"""
PPO-based Impedance Parameter Learning Policy

Implements a PPO (Proximal Policy Optimization) agent for learning
optimal impedance parameters in bimanual manipulation tasks.

The policy learns to:
1. Track desired trajectories generated by high-level policies
2. Minimize external forces during manipulation
3. Adapt impedance parameters based on task requirements

Uses Stable-Baselines3 for the PPO implementation.
"""

import numpy as np
import torch
import torch.nn as nn
from typing import Optional, Dict, Any
from stable_baselines3 import PPO
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.logger import configure
import gymnasium as gym


class ImpedanceFeatureExtractor(BaseFeaturesExtractor):
    """
    Custom feature extractor for impedance learning.

    Processes observations to extract relevant features for impedance control.
    Uses separate processing for different observation components.
    """

    # Observation indices (for clarity and maintainability)
    WRENCH_DIM = 3
    POSE_DIM = 3
    TWIST_DIM = 3
    OBS_PER_ARM = WRENCH_DIM + POSE_DIM + TWIST_DIM + POSE_DIM + TWIST_DIM  # 15
    NUM_ARMS = 2

    # Start indices for each component
    WRENCH_START = 0
    WRENCH_END = WRENCH_START + WRENCH_DIM * NUM_ARMS  # 0:6
    POSE_CURRENT_START = WRENCH_END
    POSE_CURRENT_END = POSE_CURRENT_START + POSE_DIM * NUM_ARMS  # 6:12
    TWIST_CURRENT_START = POSE_CURRENT_END
    TWIST_CURRENT_END = TWIST_CURRENT_START + TWIST_DIM * NUM_ARMS  # 12:18
    POSE_DESIRED_START = TWIST_CURRENT_END
    POSE_DESIRED_END = POSE_DESIRED_START + POSE_DIM * NUM_ARMS  # 18:24
    TWIST_DESIRED_START = POSE_DESIRED_END
    TWIST_DESIRED_END = TWIST_DESIRED_START + TWIST_DIM * NUM_ARMS  # 24:30

    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):
        """
        Initialize feature extractor.

        Args:
            observation_space: Observation space
            features_dim: Dimension of output features
        """
        super().__init__(observation_space, features_dim)

        # Observation breakdown (per arm):
        # - External wrench: 3
        # - Current pose: 3
        # - Current twist: 3
        # - Desired pose: 3
        # - Desired twist: 3
        # Total per arm: 15
        # Bimanual: 30

        obs_dim = observation_space.shape[0]
        
        # Determine dimensions dynamically
        # Assume standard structure: [wrench, current_pose, current_twist, desired_pose, desired_twist]
        # Each component is 3 * NUM_ARMS
        self.NUM_COMPONENTS = 5
        self.input_dim_per_component = obs_dim // self.NUM_COMPONENTS
        
        # Wrench encoder
        self.wrench_encoder = nn.Sequential(
            nn.Linear(self.input_dim_per_component, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU()
        )

        # Pose encoder (current + desired)
        self.pose_encoder = nn.Sequential(
            nn.Linear(self.input_dim_per_component * 2, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )

        # Twist encoder (current + desired)
        self.twist_encoder = nn.Sequential(
            nn.Linear(self.input_dim_per_component * 2, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU()
        )

        # Combine features
        combined_dim = 64 + 128 + 128  # 320
        self.combiner = nn.Sequential(
            nn.Linear(combined_dim, features_dim),
            nn.ReLU(),
            nn.Linear(features_dim, features_dim),
            nn.ReLU()
        )

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        """
        Extract features from observations.

        Args:
            observations: (batch_size, obs_dim) observations

        Returns:
            (batch_size, features_dim) features
        """
        # Split observations dynamically
        # Structure: [Wrenches | Cur Poses | Cur Twists | Des Poses | Des Twists]
        dim = self.input_dim_per_component
        
        wrenches = observations[:, 0:dim]
        current_poses = observations[:, dim:2*dim]
        current_twists = observations[:, 2*dim:3*dim]
        desired_poses = observations[:, 3*dim:4*dim]
        desired_twists = observations[:, 4*dim:5*dim]

        # Encode wrenches
        wrench_features = self.wrench_encoder(wrenches)

        # Encode poses (combine current and desired)
        pose_input = torch.cat([current_poses, desired_poses], dim=-1)
        pose_features = self.pose_encoder(pose_input)

        # Encode twists (combine current and desired)
        twist_input = torch.cat([current_twists, desired_twists], dim=-1)
        twist_features = self.twist_encoder(twist_input)

        # Combine all features
        combined = torch.cat([wrench_features, pose_features, twist_features], dim=-1)
        features = self.combiner(combined)

        return features


class ImpedanceLoggingCallback(BaseCallback):
    """
    Callback for logging impedance learning metrics.

    Logs statistics about learned impedance parameters during training.
    """

    def __init__(self, verbose: int = 0):
        super().__init__(verbose)
        self.impedance_stats = {
            'damping_mean': [],
            'damping_std': [],
            'stiffness_mean': [],
            'stiffness_std': []
        }

    def _on_step(self) -> bool:
        """Called at each step."""
        return True

    def _on_rollout_end(self) -> None:
        """Called at the end of each rollout."""
        # Get latest actions from rollout buffer
        if hasattr(self.model, 'rollout_buffer') and self.model.rollout_buffer.full:
            actions = self.model.rollout_buffer.actions

            # Actions represent impedance parameters
            # First 6 per arm: [damping (3), stiffness (3)]
            # Total 12: arm0 (6) + arm1 (6)

            # Compute statistics
            damping_mean = np.mean(actions[:, [0, 1, 2, 6, 7, 8]])
            damping_std = np.std(actions[:, [0, 1, 2, 6, 7, 8]])
            stiffness_mean = np.mean(actions[:, [3, 4, 5, 9, 10, 11]])
            stiffness_std = np.std(actions[:, [3, 4, 5, 9, 10, 11]])

            # Log to tensorboard
            self.logger.record('impedance/damping_mean', damping_mean)
            self.logger.record('impedance/damping_std', damping_std)
            self.logger.record('impedance/stiffness_mean', stiffness_mean)
            self.logger.record('impedance/stiffness_std', stiffness_std)


class PPOImpedancePolicy:
    """
    PPO-based policy for learning impedance parameters.

    Wraps Stable-Baselines3 PPO with custom feature extractor and
    provides convenient interface for training and inference.
    """

    def __init__(
        self,
        env: gym.Env,
        learning_rate: float = 3e-4,
        n_steps: int = 2048,
        batch_size: int = 64,
        n_epochs: int = 10,
        gamma: float = 0.99,
        gae_lambda: float = 0.95,
        clip_range: float = 0.2,
        ent_coef: float = 0.01,
        vf_coef: float = 0.5,
        max_grad_norm: float = 0.5,
        features_dim: int = 256,
        device: str = 'auto',
        verbose: int = 1,
        tensorboard_log: Optional[str] = None
    ):
        """
        Initialize PPO impedance policy.

        Args:
            env: Impedance learning environment
            learning_rate: Learning rate
            n_steps: Number of steps to run for each environment per update
            batch_size: Minibatch size
            n_epochs: Number of epochs when optimizing the surrogate loss
            gamma: Discount factor
            gae_lambda: Factor for trade-off of bias vs variance for GAE
            clip_range: Clipping parameter for PPO
            ent_coef: Entropy coefficient for the loss
            vf_coef: Value function coefficient for the loss
            max_grad_norm: Maximum norm for gradient clipping
            features_dim: Dimension of feature extractor output
            device: Device for computation
            verbose: Verbosity level
            tensorboard_log: Path for tensorboard logs
        """
        self.env = env
        self.features_dim = features_dim

        # Policy kwargs with custom feature extractor
        policy_kwargs = dict(
            features_extractor_class=ImpedanceFeatureExtractor,
            features_extractor_kwargs=dict(features_dim=features_dim),
            net_arch=dict(pi=[256, 256], vf=[256, 256])
        )

        # Create PPO model
        self.model = PPO(
            policy='MlpPolicy',
            env=env,
            learning_rate=learning_rate,
            n_steps=n_steps,
            batch_size=batch_size,
            n_epochs=n_epochs,
            gamma=gamma,
            gae_lambda=gae_lambda,
            clip_range=clip_range,
            ent_coef=ent_coef,
            vf_coef=vf_coef,
            max_grad_norm=max_grad_norm,
            policy_kwargs=policy_kwargs,
            verbose=verbose,
            device=device,
            tensorboard_log=tensorboard_log
        )

        # Logging callback
        self.logging_callback = ImpedanceLoggingCallback(verbose=verbose)

    def train(
        self,
        total_timesteps: int,
        callback: Optional[BaseCallback] = None,
        log_interval: int = 1,
        eval_env: Optional[gym.Env] = None,
        eval_freq: int = 10000,
        n_eval_episodes: int = 5
    ) -> 'PPOImpedancePolicy':
        """
        Train the policy.

        Args:
            total_timesteps: Total number of timesteps to train
            callback: Optional callback(s)
            log_interval: Logging interval
            eval_env: Optional evaluation environment
            eval_freq: Evaluation frequency
            n_eval_episodes: Number of evaluation episodes

        Returns:
            self
        """
        # Combine callbacks
        callbacks = [self.logging_callback]
        if callback is not None:
            if isinstance(callback, list):
                callbacks.extend(callback)
            else:
                callbacks.append(callback)

        # Add evaluation callback if eval_env is provided
        if eval_env is not None:
            from stable_baselines3.common.callbacks import EvalCallback
            eval_callback = EvalCallback(
                eval_env,
                best_model_save_path=None,
                log_path=None,
                eval_freq=eval_freq,
                n_eval_episodes=n_eval_episodes,
                deterministic=True,
                render=False
            )
            callbacks.append(eval_callback)

        # Train
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callbacks,
            log_interval=log_interval
        )

        return self

    def predict(
        self,
        observation: np.ndarray,
        deterministic: bool = True
    ) -> np.ndarray:
        """
        Predict action (impedance parameters) from observation.

        Args:
            observation: Current observation
            deterministic: Whether to use deterministic policy

        Returns:
            Action (impedance parameters)
        """
        action, _ = self.model.predict(observation, deterministic=deterministic)
        return action

    def get_impedance_parameters(
        self,
        observation: np.ndarray,
        deterministic: bool = True
    ) -> Dict[str, np.ndarray]:
        """
        Get impedance parameters from observation.

        Args:
            observation: Current observation
            deterministic: Whether to use deterministic policy

        Returns:
            Dictionary with 'damping' and 'stiffness' arrays
        """
        # Get action
        action = self.predict(observation, deterministic=deterministic)

        # Decode action to impedance parameters using environment's decoder
        if hasattr(self.env, '_decode_action'):
            return self.env._decode_action(action)
        else:
            raise ValueError("Environment does not have _decode_action method")

    def save(self, path: str):
        """
        Save policy to file.

        Args:
            path: Save path
        """
        self.model.save(path)

    def load(self, path: str, env: Optional[gym.Env] = None):
        """
        Load policy from file.

        Args:
            path: Load path
            env: Optional environment (uses self.env if not provided)
        """
        if env is None:
            env = self.env

        self.model = PPO.load(path, env=env)

    @classmethod
    def load_from_file(
        cls,
        path: str,
        env: gym.Env
    ) -> 'PPOImpedancePolicy':
        """
        Load policy from file (class method).

        Args:
            path: Load path
            env: Environment

        Returns:
            Loaded policy
        """
        policy = cls(env, verbose=0)
        policy.load(path, env)
        return policy

    def evaluate(
        self,
        n_episodes: int = 10,
        deterministic: bool = True,
        render: bool = False
    ) -> Dict[str, float]:
        """
        Evaluate policy.

        Args:
            n_episodes: Number of evaluation episodes
            deterministic: Whether to use deterministic policy
            render: Whether to render

        Returns:
            Dictionary with evaluation metrics
        """
        episode_rewards = []
        episode_lengths = []

        for _ in range(n_episodes):
            obs, _ = self.env.reset()
            done = False
            episode_reward = 0.0
            episode_length = 0

            while not done:
                action = self.predict(obs, deterministic=deterministic)
                obs, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated

                episode_reward += reward
                episode_length += 1

                if render:
                    self.env.render()

            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)

        return {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'mean_length': np.mean(episode_lengths),
            'std_length': np.std(episode_lengths)
        }
