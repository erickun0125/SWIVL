% Method
\section{Method}
\label{sec:method}

We present \textbf{SWIVL}, a hierarchical control framework that bridges high-level cognitive planning with physically grounded bimanual execution. SWIVL consists of three key components: (1) a Stable Imitation Vector Field based \textbf{Reference Twist Field Generator} that transforms discrete high-level waypoints into dense, continuous vector fields defined over the entire task space---providing stable reference motions even when the system deviates from the desired trajectory,  (2) Screw Axes Decomposition based \textbf{Twist-driven Impedance Controller} that enables practical impedance control with pose error and twist error, and (3) a Reinforcement Learning based \textbf{Wrench-feedback and Object-conditioned Impedance Variable Learning Policy} that modulates the reference motions in a physically feasible manner by explicitly incorporating object information and end-effector wrench feedback.

\textbf{Notation.} We use the following frame conventions throughout:
\begin{itemize}
    \item $\{s\}$: Spatial (world) frame
    \item $\{b_i\}$: Body frame of end-effector $i \in \{l, r\}$
    \item $\{d_i\}$: Desired frame of end-effector $i$
    \item $T_{ab}$: Transformation from frame $\{b\}$ to frame $\{a\}$
    \item ${}^a\mathcal{V}_b$: Twist of frame $\{b\}$ expressed in frame $\{a\}$
\end{itemize}

\subsection{Problem Formulation}
\label{sec:problem_formulation}

We address the problem of \textbf{bimanual manipulation of articulated objects} where two robot arms cooperatively manipulate a shared articulated object. Our goal is to develop SWIVL, a control framework that tracks high-level motion plans while ensuring physical feasibility, stable grasping, and damage prevention during bimanual interaction. This problem presents four core challenges that directly motivate SWIVL's architectural components.

\textbf{Framework Scope.} We develop the theoretical framework in \textbf{SE(3) environment} with \textbf{multi-DoF articulated objects} for generality and real-world applicability, while experimental validation is conducted in \textbf{SE(2) planar environments} with \textbf{1-DoF articulated objects} to isolate core challenges of force coupling and constraint satisfaction. The formulation targets the Franka FR3 dual-arm platform for future real-world deployment.

\subsubsection{Challenge 1: From Discrete Poses to Dense Twist References}

Modern learning-based manipulation policies---including VLAs and behavior cloning architectures---predominantly output actions in the form of \textbf{action chunks}: sequences of waypoints that specify desired end-effector trajectories over a finite horizon. This action chunking paradigm enables temporal consistency and multi-step reasoning in high-level planning.

\textbf{The Challenge.} These action chunks present critical gaps for low-level execution. First, waypoints are inherently \textbf{sparse}, providing discrete snapshots while control requires continuous dense references at high frequency. Second, their \textbf{open-loop nature} becomes particularly problematic in bimanual manipulation: while single-arm tasks like pick-and-place typically track action chunks reliably, dual-arm coordination introduces inter-arm forces and complex contact dynamics that cause significant trajectory deviations, requiring corrective feedback to guide the system back.

More fundamentally, there exists a \textbf{configuration space versus velocity space mismatch}. High-level planners naturally output poses $T \in \mathrm{SE}(3)$, but kinematic constraints and impedance control are fundamentally more tractable in the twist space $\mathfrak{se}(3)$. Kinematic constraints are linear in twist space (${}^s\mathcal{V}_l - {}^s\mathcal{V}_r = \mathbf{J}_s \dot{\mathbf{q}}_{\text{obj}}$) but nonlinear in SE(3). Similarly, SE(3) impedance control involves nonlinear Jacobians $J_{\mathcal{E}}$ that introduce configuration-dependent coupling. This mismatch motivates transforming pose commands into twist-space representations where constraints are linear and impedance modulation is geometrically consistent.

\textbf{Solution Preview.} The \textbf{Reference Twist Field Generator} addresses this challenge through three operations: (1) SE(3) trajectory smoothing to generate dense pose trajectories, (2) body twist computation to transform poses into velocity space where constraints and impedance are tractable, and (3) stable imitation vector field construction that provides continuous reference twists $\mathcal{V}^{\text{ref}}$ with pose-error-driven corrective feedback.

\subsubsection{Challenge 2: Kinematic Constraints and Compliant Coordination}

When two arms grasp and manipulate an articulated object, their motions are coupled through the object's kinematic structure. Let $T_{sb_l}, T_{sb_r} \in \mathrm{SE}(3)$ denote the end-effector poses and $\mathbf{q}_{\text{obj}} \in \mathbb{R}^k$ represent the object's internal joint configuration. The kinematic structure is characterized by the spatial Jacobian $\mathbf{J}_s(\mathbf{q}_{\text{obj}}) \in \mathbb{R}^{6 \times k}$, whose columns are spatial screw axes $\mathcal{S}_j \in \mathbb{R}^6$ for each joint.

\textbf{Holonomic Constraint.} Since both end-effectors rigidly grasp the articulated object, the relative motion between them must exactly match the motion generated by the object's internal joints:
\begin{equation}
{}^s\mathcal{V}_{b_l} - {}^s\mathcal{V}_{b_r} = \mathbf{J}_s(\mathbf{q}_{\text{obj}}) \dot{\mathbf{q}}_{\text{obj}},
\end{equation}
where ${}^s\mathcal{V}_{b_l}, {}^s\mathcal{V}_{b_r} \in \mathbb{R}^6$ are the spatial twists and $\dot{\mathbf{q}}_{\text{obj}} \in \mathbb{R}^k$ is the joint velocity vector.

\textbf{The Challenge.} A seemingly natural solution is to structurally enforce this constraint in the action space---for example, by commanding $\mathcal{V}_r$ and $\dot{\mathbf{q}}_{\text{obj}}$ as actions and automatically computing $\mathcal{V}_l = \mathcal{V}_r + \mathbf{J}_s \dot{\mathbf{q}}_{\text{obj}}$. However, this \textbf{hard constraint approach is brittle}: any small error in $\mathbf{J}_s$ due to perception noise, calibration errors, or object model mismatch directly translates to physically infeasible commanded motions. When executed via high-stiffness position or velocity control, these infeasible commands generate large internal forces that risk grasp slippage, object damage, or hardware failure.

\textbf{Solution Preview.} Rather than enforcing hard kinematic constraints, SWIVL adopts a \textbf{compliant approach} through impedance control. This enables soft regulation of constraint violations through compliance rather than rigid enforcement, providing robustness to model uncertainties while minimizing harmful internal forces.

\subsubsection{Challenge 3: Impedance Control on SE(3) and Large Trajectory Deviations}

Impedance control---regulating the dynamic relationship between motion and force---is a natural paradigm for achieving compliant manipulation. In Euclidean spaces, impedance control is straightforward: pose errors naturally define potential energy, and elastic forces emerge through simple differentiation. However, extending impedance control to the SE(3) manifold presents fundamental mathematical challenges.

\textbf{SE(3) Impedance Difficulties.} Unlike Euclidean spaces, SE(3) is a manifold where defining meaningful error metrics is non-trivial. The most physically natural metric on SE(3) is the \textbf{kinetic energy-based Riemannian metric} induced by the spatial inertia matrix $M \in \mathbb{R}^{6 \times 6}$. For tracking control, we need to measure the "distance" between the current pose $T$ and desired pose $T_{\text{des}}$, typically represented by the error pose $T_{\text{err}} = T_{\text{des}}^{-1} T$. However, SE(3) lacks a \textbf{bi-invariant Riemannian metric}---a metric that remains consistent under both left and right group actions. The exponential map follows screw motions (constant-twist trajectories), while geodesics under the kinetic energy metric follow more complex, inertia-weighted paths. This mismatch means that the natural logarithm map $\log(T_{\text{err}})^\vee \in \mathfrak{se}(3)$, commonly used to define pose errors, does not correspond to minimal-energy paths under the physically meaningful metric.
For SE(3), a nonlinear Jacobian-like operator emerges:

\begin{equation}
{\mathcal{F}_{\mathrm{elastic}} = -J_{\mathcal{E}}^\top K \mathcal{E}} \, ,
\end{equation}

where $J_{\mathcal{E}}$ is given by:
\begin{equation}
J_{\mathcal{E}} = \begin{pmatrix} \alpha J_l^{-1}(e_R) & 0_{3 \times 3} \\ -[e_p] & I_{3} \end{pmatrix} \in \mathbb{R}^{6 \times 6} \, .
\end{equation}
Here, $J_l(\theta)$ is the left Jacobian of SO(3):
\begin{equation}
J_l(\theta) = I + \frac{1 - \cos \|\theta\|}{\|\theta\|^2} [\theta] + \frac{\|\theta\| - \sin \|\theta\|}{\|\theta\|^3} [\theta]^2 \, .
\end{equation} 
$J_{\mathcal{E}}$ is configuration-dependent and couples rotational and translational components in complex, state-dependent ways. This nonlinearity fundamentally limits the design freedom of the stiffness matrix $K$

\textbf{Practical Approximations and Their Breakdown.} Commonly used impedance control formulations circumvent these issues through two assumptions: (1) \textbf{small orientation errors}, and (2) \textbf{diagonal stiffness matrices}, allowing $ J_{\mathcal{E}}^\top K \mathcal{E} \approx K \mathcal{E}$, of avoiding cross-coupling between rotation and translation. Under these assumptions, the nonlinear Jacobian's effects become negligible.

\textbf{The Challenge.} These assumptions break down in our setting. First, articulated object constraints cause \textbf{large trajectory deviations}: when high-level planners generate commands without explicit constraint awareness, the resulting pose errors can be substantial, violating the small-error assumption. Second, our motion decomposition framework (separating bulk and internal motions) requires \textbf{non-diagonal stiffness matrices} to enable independent compliance modulation along different subspaces, making diagonal-only designs insufficient.

\textbf{Solution Preview.} Rather than directly computing elastic wrenches from pose errors through the problematic $J_{\mathcal{E}}$, SWIVL adopts a \textbf{twist-driven impedance control} approach. The \textbf{Reference Twist Field} incorporates pose error $\mathcal{E}_i$ directly into the reference twist through the stability term $k_{p_i} \mathcal{E}_i$, converting pose deviations into corrective velocity commands. The controller then performs impedance regulation in twist space, where damping naturally relates velocity errors to wrenches through linear operators, sidestepping the nonlinear Jacobian while maintaining compliant tracking behavior.

\subsubsection{Challenge 4: Motion Decomposition and Wrench-Based Coordination}

For task-agnostic control, the low-level policy must be trained solely on trajectory tracking rewards rather than task-specific success metrics, enabling the same policy to track diverse high-level commands without retraining for each task objective.

\textbf{Object-Centric Motion Decomposition.} In articulated object bimanual manipulation, tracking is fundamentally object-centric. Rather than independently tracking each arm's reference motion $\mathcal{V}_l^{\text{ref}}$ and $\mathcal{V}_r^{\text{ref}}$, it is advantageous to decompose these into:
\begin{itemize}
    \item \textbf{Internal motion}: Drives joint articulation, lies in $\text{range}(\mathbf{J}_s(\mathbf{q}_{\text{obj}}))$
    \item \textbf{Bulk motion}: Drives the object's overall motion through space, orthogonal to  $\text{range}(\mathbf{J}_s(\mathbf{q}_{\text{obj}}))$
\end{itemize}
Tracking these components separately provides clearer learning signals aligned with the object's kinematic structure.
\begin{align}
\mathcal{V}_{i,\parallel}^{\text{ref}} &= P_{i,\parallel} \mathcal{V}_i^{\text{ref}}, \quad \mathcal{V}_{i,\parallel} = P_{i,\parallel} \mathcal{V}_i \quad \text{(internal motion)}, \\
\mathcal{V}_{i,\perp}^{\text{ref}} &= P_{i,\perp} \mathcal{V}_i^{\text{ref}}, \quad \mathcal{V}_{i,\perp} = P_{i,\perp} \mathcal{V}_i \quad \text{(bulk motion)},
\end{align}

\textbf{The Challenge: Bulk Motion Ambiguity.} If the high-level policy generated kinematically consistent commands, then after decomposing each arm's motion, their bulk components would naturally agree. However, high-level planners lack explicit constraint awareness, resulting in $\mathcal{V}_l^{\text{ref}}$ and $\mathcal{V}_r^{\text{ref}}$ whose bulk motions \textbf{do not match}. This creates a fundamental ambiguity: which arm's bulk motion should the policy track?

This ambiguity manifests as \textbf{inter-arm force coupling}. Internal motion components naturally satisfy kinematic constraints and generate minimal internal forces. However, bulk motion mismatch directly translates into inter-arm wrenches measured by force-torque sensors. Specifically, wrench components orthogonal to the screw axes---the non-productive components that do not contribute to joint actuation---directly reflect the magnitude of bulk motion disagreement. By decomposing measured wrenches as:
\begin{equation}
\mathcal{F}_{i,\perp} = \mathbf{P}_{i,\perp}^T \mathcal{F}_i, \quad \mathcal{F}_{i,\parallel} = \mathbf{P}_{i,\parallel}^T \mathcal{F}_i,
\end{equation}
we can quantify internal forces $\mathcal{F}_{i,\perp}$ arising from coordination errors. Minimizing $\|\mathcal{F}_{i,\perp}\|^2$ implicitly resolves the bulk motion ambiguity: the policy learns to track reference motions in a manner that achieves coordinated bulk motion while maintaining compliant internal motion.

\textbf{Solution Preview.} This challenge is addressed through \textbf{wrench-feedback conditioned RL} combined with \textbf{learned impedance variable modulation}. The \textbf{Impedance Variable Learning Policy} receives wrench decompositions and learns to adaptively modulate damping coefficients $d_{i,\parallel}$ and $d_{i,\perp}$ for internal versus bulk motions. The \textbf{Reward Design} explicitly penalizes internal wrenches $\|\mathcal{F}_{i,\perp}\|^2$, training the policy to minimize non-productive forces while tracking reference trajectories. This approach enables simple reference tracking to implicitly achieve decomposed motion coordination through force-compliant behavior.

\subsection{SWIVL Architecture}

SWIVL adopts a \textbf{four-layer hierarchical architecture} that directly addresses the four challenges identified above. The architecture decouples high-level reasoning from low-level physical interaction:

\begin{itemize}
    \item \textbf{Layer 1 (High-Level Policy)}: Provides task-level guidance through sparse waypoint generation
    \item \textbf{Layer 2 (Reference Twist Field Generator)}: Addresses Challenge 1 by transforming sparse waypoints into dense, closed-loop reference trajectories
    \item \textbf{Layer 3 (Impedance Variable Modulation Policy)}: Addresses Challenges 3 \& 4 by learning adaptive compliance modulation based on object geometry and wrench feedback
    \item \textbf{Layer 4 (Screw-decomposed Controller)}: Addresses Challenges 2 \& 3 by structurally enforcing kinematic constraints through orthogonal motion decomposition
\end{itemize}

The core innovation lies in the tight integration of Layers 2-4, which together enable physically grounded, force-compliant bimanual manipulation underneath arbitrary high-level planners.

\subsubsection{Layer 1: High-Level Policy}

The top layer generates goal-directed behavior in action chunk. This layer can be instantiated by:
\begin{itemize}
    \item \textbf{Vision-Language-Action (VLA) models}
    \item \textbf{Behavior cloning policies}
    \item \textbf{Teleoperation interfaces}
\end{itemize}

\textbf{Interface:} Outputs discrete end-effector pose waypoints $\lbrace T_{sd_l}[\tau], T_{sd_r}[\tau] \rbrace_{\tau=0}^{H}$ at low frequency.

\subsubsection{Layer 2: Reference Twist Field Generator}

This layer addresses Challenge 1 by bridging the gap between discrete high-level waypoints and continuous low-level control. High-level policies provide sparse waypoints at low frequency ($\sim$10Hz), while the low-level controller requires smooth, dense reference trajectories at high frequency ($\sim$50Hz) with corrective feedback capabilities. The Reference Twist Field Generator performs three steps to achieve this transformation:

\paragraph{Step 1: SE(3) Trajectory Smoothing.} To address the sparsity gap, we perform smooth interpolation in $SE(3)$ to obtain dense desired trajectories at the Low-Level Policy frequency $\Delta t_{LL} \ll \Delta t_{HL}$:

\begin{equation}
\lbrace T_{sd_l}(t), T_{sd_r}(t) \rbrace_{t=0}^{H_{LL}},
\end{equation}

where $H_{LL}$ is the smoothed trajectory horizon. We use SLERP for rotations and cubic splines for translations; see Appendix~\ref{app:trajectory_interpolation} for details. Both interpolation schemes are differentiable in time, so they induce smooth position and rotation trajectories $p_i^{\text{des}}(t)$ and $R_i^{\text{des}}(t)$ for each end-effector.

\paragraph{Step 2: Body Twist Computation.} For a smooth trajectory $T_{sd_i}(t) = \begin{bmatrix} R_{sd_i}(t) & p_{sd_i}(t) \\ 0 & 1 \end{bmatrix}$, we directly compute the desired twist by differentiating the pose and expressing the resulting velocities in the instantaneous desired frame. Let $R_{sd_i}(t)$ denote the rotation from the desired body frame $\{d_i\}$ to the spatial frame $\{s\}$, and $p_{sd_i}(t)$ the position of the desired body frame origin in the spatial frame. The corresponding desired-frame angular and linear velocities are given by

\begin{equation}
\label{eq:body_twist}
\mathcal{V}_i^{\text{des}}(t) =
\begin{bmatrix}
\omega_i^{\text{des}}(t) \\
v_i^{\text{des}}(t)
\end{bmatrix},
\quad [\omega_i^{\text{des}}(t)]_\times = R_{sd_i}(t)^{\top} \dot{R}_{sd_i}(t),
\quad v_i^{\text{des}}(t) = R_{sd_i}(t)^{\top} \dot{p}_{sd_i}(t).
\end{equation}

Here $[\omega]_\times$ is the skew-symmetric matrix representation of the angular velocity vector $\omega$, and both $\omega_i^{\text{des}}(t)$ and $v_i^{\text{des}}(t)$ are expressed in the desired frame $\{d_i\}$ by construction. 

\paragraph{Step 3: Stable Imitation Vector Field Design.} 
As execution progresses within an action chunk, the actual end-effector poses may deviate significantly from the desired trajectory due to tracking errors, disturbances, and model mismatch. 
To address the open-loop nature of waypoints and provide robustness when the system deviates from the desired trajectory, we construct a vector field that balances \textbf{imitation} of demonstrated motions and \textbf{stability} for error correction.
We employ a stable imitation vector field that combines two components: an imitation term that mimics the demonstrated velocity profile at the temporally synchronized point, and a stability term that provides corrective feedback via a term proportional to the SE(3) pose error for convergence to the desired trajectory:

\begin{equation}
\label{eq:vector_field}
\mathcal{V}_i^{\text{ref}}(t, T_{sb_i}) = \mathrm{Ad}_{T_{b_id_i}}\mathcal{V}_i^{\text{des}}(t) + k_{p_i} \mathcal{E},
\end{equation}
where $\mathrm{Ad}_T$ denotes the adjoint transformation that maps twists between frames. Since the desired twist $\mathcal{V}_i^{\text{des}}(t)$ is computed in the desired frame $\{d_i\}$ (Eq.~\ref{eq:body_twist}), we must transform it to the current body frame $\{b_i\}$ where the controller operates. The transformation $T_{b_id_i} = T_{b_is} T_{sd_i} = (T_{sb_i})^{-1} T_{sd_i}$ represents the relative transformation from the desired frame to the current body frame, and $\mathrm{Ad}_{T_{b_id_i}}$ performs the corresponding twist transformation. The pose error term is given by:
\begin{equation}
\begin{aligned}
\mathcal{E} &= \begin{pmatrix} \alpha e_R \\ e_p \end{pmatrix} \in \mathbb{R}^6 \\
e_{R_i} &= \log(R_{sb_i}^\top R_{sd_i})^\vee \in \mathbb{R}^3 \quad \text{(rotation error in body frame)} \\
e_{p_i} &= R_{sb_i}^\top(p_{sd_i} - p_{sb_i}) \in \mathbb{R}^3 \quad \text{(translation error in body frame)} \\
k_{p_i} &\in \mathbb{R} \quad \text{(scalar proportional gain)} \\
\end{aligned}
\end{equation}
Here, $\alpha$ represents a characteristic length that weights the rotational cost relative to translation, where $G = \mathrm{diag}(\alpha^2 I_3, I_3)$ is the metric tensor defining an inner product $\langle \cdot, \cdot \rangle_G$ on $\mathfrak{se}(3)$:
\begin{equation}
\langle \mathcal{V}_1, \mathcal{V}_2 \rangle_G = \frac{\alpha^2}{2} \mathrm{tr}([\omega_1]^\top [\omega_2]) + v_1^\top v_2 =  \mathcal{V}_1^\top G \mathcal{V}_2 \, ,
\end{equation}
where $\mathcal{V}_i = \begin{bmatrix} \omega_i \\ v_i \end{bmatrix}$.
This time-based approach offers computational efficiency ($O(1)$ lookup) while maintaining strong tracking performance for smooth bimanual trajectories. The term $k_p \mathcal{E}$, as will be described later, plays a role in creating an elastic force for impedance control without using the nonlinear Jacobian matrix to create a reference twist.

\subsubsection{Layer 3: Impedance Variable Modulation Policy}

This layer addresses Challenges 3 and 4 by learning adaptive impedance modulation. The Reinforcement Learning Policy $\pi_\theta: \mathcal{O} \to \mathcal{A}$ modulates impedance variables to enable physically feasible motions while accounting for object constraints and inter-arm force interactions. By explicitly conditioning on object geometry (screw axes) and wrench feedback, the policy learns to independently modulate compliance for bulk versus internal motions and minimize harmful internal forces.

\textbf{Observation Space $\mathcal{O}$.} The policy receives:

\begin{enumerate}
    \item \textbf{Reference twists}: $\lbrace \mathcal{V}_l^{\text{ref}}, \mathcal{V}_r^{\text{ref}} \rbrace \in \mathfrak{se}(3) \times \mathfrak{se}(3)$
    These are the reference motions computed by the Reference Twist Field Generator (Layer 2) at the current time $t$ and current end-effector poses $T_{sb_l}, T_{sb_r}$.

    \item \textbf{Object constraints}: $\lbrace \mathcal{B}_l, \mathcal{B}_r \rbrace \in \mathfrak{se}(3) \times \mathfrak{se}(3)$
    These encode the kinematic constraint of the manipulated object. $\mathcal{B}_l, \mathcal{B}_r$ are the body-frame screw axes defining the object's allowable internal motion directions at each end-effector. For articulated objects, these correspond to the columns of the object Jacobian $J_i$, related to the spatial screw axis via $\mathcal{B}_i = \mathrm{Ad}_{T_{b_is}} \mathcal{S}$ where $\mathrm{Ad}_{T_{b_is}}$ transforms spatial twists to body frame.

    \item \textbf{Wrench feedback}: $\lbrace \mathcal{F}_l, \mathcal{F}_r \rbrace \in \mathfrak{se}(3)^* \times \mathfrak{se}(3)^*$
    These are 6-dimensional wrench measurements (3D moment + 3D force) obtained from 6-axis force-torque sensors mounted at each end-effector's wrist. Raw sensor data may be filtered (e.g., low-pass filtering or exponential smoothing) when necessary to reduce measurement noise while preserving force feedback responsiveness for compliant control.

    \item \textbf{Proprioception}: $\lbrace T_{sb_l}, T_{sb_r}, \mathcal{V}_l, \mathcal{V}_r \rbrace$
    Task-space states including end-effector poses $T_{sb_l}, T_{sb_r} \in \mathrm{SE}(3)$ and body twists $\mathcal{V}_l, \mathcal{V}_r \in \mathfrak{se}(3)$.
\end{enumerate}

\textbf{Action Space $\mathcal{A}$.} We propose an impedance variable action space that structurally enforces object's kinematic constraints by motion decomposition. These variables parameterize the low-level controller (detailed in Section~\ref{sec:layer4_controller}), allowing the policy to modulate compliance behavior dynamically:
\begin{equation}
a_t = ( d_{l,\parallel},  d_{r,\parallel}, d_{l,\perp},  d_{r,\perp},k_{p_l},  k_{p_r},\alpha) \in \mathbb{R}^7,
\end{equation}
where $d_{i,\parallel}, d_{i,\perp}, k_{p_i}, \alpha \in \mathbb{R}^+$ are positive scalar gains, and:
\begin{itemize}
    \item $ d_{i,\parallel}$: Damping coefficient for internal motion (parallel to screw axis), regulating compliance along the object's degree of freedom.
    \item $ d_{i,\perp}$: Damping coefficient for bulk motion (orthogonal to screw axis), regulating compliance for the object's overall transport.
    \item $k_{p_i}$: Stiffness gain for the stability term in the vector field, determining the strength of correction towards the desired trajectory.
    \item $\alpha$: \textbf{Learnable characteristic length scale} that adaptively weights rotational error relative to translational error in the SE(3) metric tensor $G = \mathrm{diag}(\alpha^2 I_3, I_3)$. By learning $\alpha$, the policy discovers task-appropriate metric structures for orthogonal decomposition and compliance modulation.
\end{itemize}

These policy outputs parameterize the low-level controller (detailed in Layer 4 below), enabling adaptive, context-dependent impedance modulation.

\subsubsection{Layer 4: Screw-decomposed Twist-driven Impedance Controller}
\label{sec:layer4_controller}

This layer addresses Challenges 2 and 3 by executing low-level control that structurally enforces kinematic constraints while enabling independent compliance modulation. It tracks the reference motion $\mathcal{V}_i^{\text{ref}}$ from Layer 2 using the impedance parameters from Layer 3. The key innovation of this controller is its ability to provide \textbf{two complementary views}: (1) it behaves as a geometrically consistent SE(3) impedance controller, and (2) it explicitly decomposes control actions into bulk and internal motion spaces, enabling task-aware compliance modulation.

\textbf{Orthogonal Decomposition via Object Jacobian.} To enable motion decomposition, we first establish projection operators that separate motions into components parallel and orthogonal to the object's kinematic constraints. Let $J_i(\mathbf{q}_{\text{obj}}) \in \mathbb{R}^{6 \times k}$ denote the body Jacobian of the object for end-effector $i \in \{l, r\}$, which encodes how the object's joint velocities $\dot{\mathbf{q}}_{\text{obj}}$ manifest as end-effector body twists. The body Jacobian relates to the spatial Jacobian (Eq.~2) via the adjoint transformation: $J_i = \mathrm{Ad}_{T_{b_is}} J_s$, where $T_{b_is} = (T_{sb_i})^{-1}$ transforms spatial frame quantities to the body frame.

Using the inner product $\langle \mathcal{V}_1, \mathcal{V}_2 \rangle_G = \mathcal{V}_1^T G \mathcal{V}_2$ with metric tensor $G = \mathrm{diag}(\alpha^2 I_3, I_3)$ on $\mathfrak{se}(3)$, we construct orthogonal projection operators:

\begin{equation}
\label{eq:projection_operators}
\begin{aligned}
P_{i,\parallel} &= J_i (J_i^\top G J_i)^{-1} J_i^\top G, \quad P_{i,\perp} = I - P_{i,\parallel},
\end{aligned}
\end{equation}

where $P_{i,\parallel}$ projects onto the internal motion subspace (range of $J_i$) and $P_{i,\perp}$ projects onto the bulk motion subspace (orthogonal complement). These projectors satisfy $P_{i,\parallel}^T G = G P_{i,\parallel}$ and $P_{i,\perp}^T G = G P_{i,\perp}$, ensuring geometric consistency under the chosen metric. Importantly, since $\alpha$ is learned by the policy (Layer 3), the metric tensor $G$ and hence the projection operators adapt dynamically to task requirements, enabling context-dependent orthogonal decomposition.

\textbf{Controller Formulation.} With the learned impedance variables $a_t = (d_{l,\parallel}, d_{r,\parallel}, d_{l,\perp}, d_{r,\perp}, k_{p_l}, k_{p_r}, \alpha)$ from Layer 3, we construct a damping matrix that respects the motion decomposition:

\begin{equation}
\label{eq:damping_matrix}
K_{d_i} = G (P_{i,\parallel} d_{i,\parallel} + P_{i,\perp} d_{i,\perp}),
\end{equation}

This structure allows independent damping modulation for internal motion (via $d_{i,\parallel}$) and bulk motion (via $d_{i,\perp}$), enabling the policy to adaptively regulate compliance based on task requirements and force feedback.

The commanded wrench is then computed as:

\begin{equation}
\label{eq:impedance_control_main}
\begin{aligned}
\mathcal{F}_{\mathrm{cmd}, i} &= K_{d_i} (\mathcal{V}_i^{\text{ref}} - \mathcal{V}_i) + \mu_{b,i} + \gamma_{b,i}\\
&= K_{d_i} (\mathrm{Ad}_{T_{b_id_i}} \mathcal{V}_i^{\text{des}} - \mathcal{V}_i + k_{p_i} \mathcal{E}_i) + \mu_{b,i} + \gamma_{b,i},
\end{aligned}
\end{equation}

where $\mathcal{V}_i^{\text{ref}} = \mathrm{Ad}_{T_{b_id_i}} \mathcal{V}_i^{\text{des}} + k_{p_i} \mathcal{E}_i$ is the reference twist from Layer 2, $\mu_{b,i}$ accounts for nonlinear dynamics (Coriolis and centrifugal terms), and $\gamma_{b,i}$ provides gravity compensation (omitted in planar SE(2) settings where gravity is orthogonal to the motion plane). 

The commanded wrench is then mapped to joint-space motor torques via the manipulator Jacobian:

\begin{equation}
\label{eq:joint_torque}
\tau_{\mathrm{cmd}, i} = J_i(\theta_i)^T \mathcal{F}_{\mathrm{cmd}, i},
\end{equation}

where $J_i(\theta_i) \in \mathbb{R}^{6 \times n}$ is the geometric Jacobian of the $i$-th manipulator mapping joint velocities to end-effector twist, and $\tau_{\mathrm{cmd}, i} \in \mathbb{R}^n$ is the commanded joint torque vector that serves as the final robot control command.

We now provide two complementary interpretations that reveal the dual nature of this controller.

\textbf{Interpretation 1: SE(3) Impedance Control Structure.} 

The first interpretation reveals that our controller naturally implements SE(3) impedance control. Classical impedance control on SE(3) designs a virtual dynamical system with desired impedance characteristics:

\begin{equation}
M \dot{\xi} + D \xi + J_{\mathcal{E}}^\top K \mathcal{E} = \mathcal{F}_{\mathrm{ext}},
\end{equation}

where $\xi = {}^b\mathcal{V}_d - {}^b\mathcal{V}_b$ is the velocity error, $M$ is the desired inertia, $D$ is damping, and $J_{\mathcal{E}}^\top K \mathcal{E}$ is the nonlinear stiffness term arising from SE(3) geometry (Section~\ref{sec:problem_formulation}). The corresponding impedance controller takes the form:

\begin{equation}
\mathcal{F}_{\mathrm{cmd}} = \Lambda_b M^{-1}(D \xi + J_{\mathcal{E}}^\top K \mathcal{E}) + \Lambda_b {}^b\dot{\mathcal{V}}_d + \mu_b + \gamma_b + (I - \Lambda_b M^{-1})\mathcal{F}_{\mathrm{ext}},
\end{equation}

where $\Lambda_b$ is the operational space inertia matrix. Under the common simplifications $M = \Lambda_b$ (match desired and actual inertia) and ${}^b\dot{\mathcal{V}}_d = 0$ (constant reference velocity), this reduces to:

\begin{equation}
\mathcal{F}_{\mathrm{cmd}} = D \xi + J_{\mathcal{E}}^\top K \mathcal{E} + \mu_{b} + \gamma_{b}.
\end{equation}

Our controller in Eq.~\eqref{eq:impedance_control_main} follows this exact structure. Defining the velocity error as $\xi = \mathrm{Ad}_{T_{b_id_i}} \mathcal{V}_i^{\text{des}} - \mathcal{V}_i$, we can rewrite:

\begin{equation}
\label{eq:impedance_equivalence}
\begin{aligned}
\mathcal{F}_{\mathrm{cmd}, i} &= K_{d_i} (\mathrm{Ad}_{T_{b_id_i}} \mathcal{V}_i^{\text{des}} - \mathcal{V}_i + k_{p_i} \mathcal{E}_i) + \mu_{b,i} + \gamma_{b,i}\\
&= K_{d_i} \xi + K_{d_i} k_{p_i} \mathcal{E}_i + \mu_{b,i} + \gamma_{b,i}\\
&\approx D \xi + J_{\mathcal{E}}^\top K \mathcal{E} + \mu_{b} + \gamma_{b},
\end{aligned}
\end{equation}

where the correspondence is: $D \leftrightarrow K_{d_i}$ (learned damping) and the term $K_{d_i} k_{p_i} \mathcal{E}_i$ plays the role of $J_{\mathcal{E}}^\top K \mathcal{E}$ (stiffness). Critically, our approach \textbf{sidesteps the explicit nonlinear Jacobian $J_{\mathcal{E}}$} by incorporating the $k_{p_i} \mathcal{E}_i$ term directly into the reference twist (Layer 2), avoiding the geometric complications discussed in Section~\ref{sec:problem_formulation} while maintaining impedance behavior.

\textbf{Interpretation 2: Explicit Bulk-Internal Motion Decomposition.} 

The second interpretation reveals how the controller naturally decomposes control actions into semantically meaningful components aligned with task requirements. By decomposing both reference and actual twists into components parallel (internal) and orthogonal (bulk) to the object's kinematic constraints:

\begin{align}
\mathcal{V}_{i,\parallel}^{\text{ref}} &= P_{i,\parallel} \mathcal{V}_i^{\text{ref}}, \quad \mathcal{V}_{i,\parallel} = P_{i,\parallel} \mathcal{V}_i \quad \text{(internal motion)}, \\
\mathcal{V}_{i,\perp}^{\text{ref}} &= P_{i,\perp} \mathcal{V}_i^{\text{ref}}, \quad \mathcal{V}_{i,\perp} = P_{i,\perp} \mathcal{V}_i \quad \text{(bulk motion)},
\end{align}

we can expand the control law to reveal independent regulation of each motion component:

\begin{equation}
\label{eq:decomposed_control}
\begin{aligned}
\mathcal{F}_{\mathrm{cmd}, i} &= K_{d_i} (\mathcal{V}_i^{\text{ref}} - \mathcal{V}_i) + \mu_{b,i} + \gamma_{b,i}\\
&= G(P_{i,\parallel} d_{i,\parallel} + P_{i,\perp} d_{i,\perp})(\mathcal{V}_i^{\text{ref}} - \mathcal{V}_i) + \mu_{b,i} + \gamma_{b,i}\\
&= \underbrace{d_{i,\parallel} G (\mathcal{V}_{i,\parallel}^{\text{ref}} - \mathcal{V}_{i,\parallel})}_{\text{internal motion control}} + \underbrace{d_{i,\perp} G (\mathcal{V}_{i,\perp}^{\text{ref}} - \mathcal{V}_{i,\perp})}_{\text{bulk motion control}} + \mu_{b,i} + \gamma_{b,i}.
\end{aligned}
\end{equation}

This decomposition provides three critical properties:

\begin{enumerate}
    \item \textbf{Independent compliance modulation}: The policy can independently adjust $d_{i,\parallel}$ and $d_{i,\perp}$ to achieve task-specific compliance---high stiffness for bulk motion during transport, high compliance for internal motion during articulation, or vice versa.
    
    \item \textbf{Decoupled Power Generation}: The feedback wrenches for internal and bulk motions are \textbf{reciprocally orthogonal} to the opposing motion subspaces, ensuring zero interference in terms of virtual power:
    \begin{align}
 \mathcal{F}_{\mathrm{cmd,fb}, i,\parallel} &= d_{i,\parallel}G (\mathcal{V}_{i,\parallel}^{\text{ref}}-\mathcal{V}_{i,\parallel}) \\
\mathcal{F}_{\mathrm{cmd,fb}, i,\perp} &= d_{i,\perp} G (\mathcal{V}_{i,\perp}^{\text{ref}}-\mathcal{V}_{i,\perp})\\
(\mathcal{F}_{\mathrm{cmd,fb}, i,\parallel} )^\top (\mathcal{V}_{i,\perp}^{\text{ref}}-\mathcal{V}_{i,\perp}) &= 0\\
(\mathcal{F}_{\mathrm{cmd,fb}, i,\perp} )^\top (\mathcal{V}_{i,\parallel}^{\text{ref}}-\mathcal{V}_{i,\parallel}) &= 0 \, .
    \end{align}
    This orthogonality follows directly from the projection properties: $P_{i,\parallel}^T G P_{i,\perp} = 0$, ensuring that control actions for each motion type do not interfere with each other.
    
    \item \textbf{Constraint satisfaction}: The internal motion component $\mathcal{V}_{i,\parallel}$ automatically lies in the range of $J_i$, ensuring that commanded motions respect the object's kinematic constraints and minimize harmful internal forces.
\end{enumerate}

Together, these two interpretations demonstrate that our controller simultaneously achieves geometrically consistent SE(3) impedance behavior while enabling explicit, learning-based modulation of task-semantic motion components---a capability that would be intractable to design analytically given the geometric constraints discussed in Section~\ref{sec:problem_formulation}.


\subsection{Learning Framework}

\subsubsection{Reinforcement Learning Formulation}

We formulate the Low-Level Policy learning as a Partially Observable Markov Decision Process (POMDP) $\mathcal{M} = (\mathcal{S}, \mathcal{O}, \mathcal{A}, P, r, \gamma)$, where:
\begin{itemize}
    \item $\mathcal{S}$: State space (full environment state)
    \item $\mathcal{O}$: Observation space (partial observations available to the policy)
    \item $\mathcal{A}$: Action space
    \item $P(s_{t+1} | s_t, a_t)$: Transition dynamics
    \item $r: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$: Reward function (Section~\ref{sec:reward_design})
    \item $\gamma$: Discount factor
\end{itemize}

The objective is to maximize expected return:

\begin{equation}
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \gamma^t r(s_t, a_t) \right].
\end{equation}

\subsubsection{Reward Function Design}
\label{sec:reward_design}

The reward function balances three objectives for stable, force-compliant manipulation:

\begin{equation}
\label{eq:reward}
r_t = r_{\text{track}} + r_{\text{safety}} + r_{\text{reg}}.
\end{equation}

\textbf{Motion Tracking ($r_{\text{track}}$).} Ensures accurate tracking of reference trajectories generated by the motion field:

\begin{align}
\label{eq:reward_tracking}
r_{\text{track}} = -w_{\text{track}}\sum_{i \in \lbrace l,r \rbrace} \|\mathcal{V}_i - \mathcal{V}_i^{\text{ref}}\|_G^2 = -w_{\text{track}}\sum_{i \in \lbrace l,r \rbrace} (\mathcal{V}_i - \mathcal{V}_i^{\text{ref}})^T G (\mathcal{V}_i - \mathcal{V}_i^{\text{ref}}),
\end{align}

where $G = \mathrm{diag}(\alpha^2 I_3, I_3)$ is the learned metric tensor. This reward encourages each end-effector to follow its reference twist, where $\mathcal{V}_l, \mathcal{V}_r$ are the actual body twists and $\mathcal{V}_l^{\text{ref}}, \mathcal{V}_r^{\text{ref}}$ are the reference twists from the motion field. Using the G-metric ensures that tracking error is measured consistently with the impedance control framework, with adaptive weighting between rotational and translational components via the learned parameter $\alpha$.

\textbf{Safety ($r_{\text{safety}}$).} Ensures safe operation and minimizes non-productive internal forces:

\begin{align}
\label{eq:reward_safety}
r_{\text{safety}} = -w_{\text{int}} \sum_{i \in \lbrace l,r \rbrace} \|\mathcal{F}_{i,\perp}\|_2^2,
\end{align}

where $\mathcal{F}_i = \begin{bmatrix} m_i \\ f_i \end{bmatrix}$ is the measured wrench at each end-effector $i$.

This reward addresses the internal wrench problem identified in Section~\ref{sec:problem_formulation} by minimizing internal wrenches---wrench components orthogonal to the object's allowable motion direction. To decompose measured wrenches consistently with the twist decomposition framework in Layer 4, we seek wrench components $\mathcal{F}_{i,\parallel}$ and $\mathcal{F}_{i,\perp}$ such that they are orthogonal to complementary twist subspaces under the reciprocal product (virtual power). Specifically, we require:

\begin{align}
\langle \mathcal{F}_{i,\parallel}, \mathcal{V} \rangle &= \mathcal{F}_{i,\parallel}^T \mathcal{V} = 0 \quad \forall \mathcal{V} \in \text{range}(P_{i,\perp}), \\
\langle \mathcal{F}_{i,\perp}, \mathcal{V} \rangle &= \mathcal{F}_{i,\perp}^T \mathcal{V} = 0 \quad \forall \mathcal{V} \in \text{range}(P_{i,\parallel}),
\end{align}

where $\langle \mathcal{F}, \mathcal{V} \rangle = \mathcal{F}^T \mathcal{V}$ is the reciprocal product representing virtual power. This orthogonality condition is naturally satisfied by projecting the measured wrench using the transpose of the twist projection operators, exploiting the dual relationship between twist and wrench spaces:

\begin{equation}
\mathcal{F}_{i,\parallel} = P_{i,\parallel}^T \mathcal{F}_i, \quad \mathcal{F}_{i,\perp} = P_{i,\perp}^T \mathcal{F}_i = (I - P_{i,\parallel})^T \mathcal{F}_i.
\end{equation}

To verify orthogonality, for any $\mathcal{V} \in \text{range}(P_{i,\perp})$, we have $\mathcal{V} = P_{i,\perp} \mathcal{V}'$ for some $\mathcal{V}'$, and:
\begin{align}
\mathcal{F}_{i,\parallel}^T \mathcal{V} &= (P_{i,\parallel}^T \mathcal{F}_i)^T (P_{i,\perp} \mathcal{V}') = \mathcal{F}_i^T P_{i,\parallel} P_{i,\perp} \mathcal{V}' = 0,
\end{align}
where the last equality follows immediately from the orthogonality of the twist projectors ($P_{i,\parallel} P_{i,\perp} = 0$). Similarly, it can be shown that $\mathcal{F}_{i,\perp}^T \mathcal{V} = 0$ for all $\mathcal{V} \in \text{range}(P_{i,\parallel})$.

The parallel component $\mathcal{F}_{i,\parallel}$ represents productive wrench that performs work along the object's internal degree of freedom, contributing to desired joint motion. The orthogonal component $\mathcal{F}_{i,\perp}$ represents \textbf{internal wrench} that:
\begin{itemize}
    \item Does not contribute to desired object motion along the screw axis (zero virtual power along $\text{range}(P_{i,\parallel})$)
    \item Arises from coordination errors between the two arms
    \item Represents constraint forces (bearing loads, friction, etc.) unrelated to joint actuation
    \item Increases unnecessary contact stress and grasp instability
    \item Wastes energy and risks hardware damage
\end{itemize}

By penalizing $\|\mathcal{F}_{i,\perp}\|_2^2$, the policy learns to minimize non-productive forces while maintaining necessary productive forces for manipulation. This wrench decomposition is fully consistent with the twist decomposition framework, utilizing the duality of the learned kinematic structure.

\textbf{Regularization ($r_{\text{reg}}$).} Encourages smooth motion:

\begin{equation}
\label{eq:reward_regularization}
r_{\text{reg}} = - w_{reg} \sum_{i \in \lbrace l,r \rbrace} \|\dot{\mathcal{V}}_i\|^2.
\end{equation}

This reduces energy consumption (torque magnitude), joint jerkiness (joint acceleration), and Cartesian jerkiness (twist acceleration), promoting natural and efficient movements.

\textbf{Termination Conditions.} To ensure grasp stability, episodes terminate early (task failure) if grasp drift exceeds safety thresholds:

\begin{equation}
\text{Terminate if: } \exists i \in \{l, r\} \text{ such that } \left\|\left[\log\left((T_{\text{grip},i}^{\text{init}})^{-1} T_{\text{grip},i}\right)\right]^\vee\right\|_2 > d_{\max},
\end{equation}

where $T_{\text{grip},i}^{\text{init}}$ is the initial grasp pose, $T_{\text{grip},i}$ is the current grasp pose, and $d_{\max}$ is the maximum allowable drift threshold. This geodesic distance on SE(3) captures both translational and rotational drift from the initial grasp configuration. When this threshold is exceeded, the episode terminates immediately with a failure signal, encouraging the policy to maintain stable grasps throughout manipulation without explicit reward shaping.

\subsubsection{Policy Network Architecture}

The Low-Level Policy $\pi_\theta: \mathcal{S} \to \mathcal{A}$ is implemented as a neural network with \textbf{object-conditioned multi-stream architecture}. The network employs Feature-wise Linear Modulation (FiLM) to inject object geometric structure into all feature processing stages, enabling constraint-aware representation learning.

For detailed architecture specifications, see Appendix~\ref{app:network_architecture}.

\subsubsection{Training Procedure}

We employ \textbf{Proximal Policy Optimization (PPO)} with standard hyperparameters for policy gradient updates.

\textbf{PPO Objective:}

\begin{equation}
\label{eq:ppo}
L^{CLIP}(\theta) = \mathbb{E}_t \left[\min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t\right)\right],
\end{equation}

where $r_t(\theta) = \pi_\theta(a_t|o_t) / \pi_{\theta_{\text{old}}}(a_t|o_t)$ and advantages are computed via Generalized Advantage Estimation (GAE):

\begin{equation}
\label{eq:gae}
\hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}, \quad \delta_t = r_t + \gamma V_\phi(o_{t+1}) - V_\phi(o_t).
\end{equation}

where $V_\phi: \mathcal{O} \to \mathbb{R}$ is the value function approximated by a separate critic network with architecture identical to the policy encoder (shared encoders, separate value head).

\textbf{Algorithm Summary:} The complete training procedure is summarized in Algorithm~\ref{alg:SWIVL_training}.

\begin{algorithm}[t]
\caption{SWIVL Training}
\label{alg:SWIVL_training}
\begin{algorithmic}[1]
\REQUIRE Pre-trained High-Level Policy $\pi_{HL}$, object set $\mathcal{O}_{\text{obj}}$
\STATE Initialize: Low-Level Policy parameters $\theta$, value function parameters $\phi$
\FOR{episode = 1 to $N_{\text{episodes}}$}
    \STATE Sample object with task $o \sim \mathcal{O}_{\text{obj}}$
    \STATE Initialize robot, environment, and action chunk buffer
        \FOR{$t = 1$ to $H$}
            \STATE \textbf{High-Level Policy}
            \IF{$t \mod f_{HL}^{-1} == 0$}
                \STATE Generate action chunk: $\{T_{sd_i}[\tau]\}_{\tau=0}^{H_{\text{chunk}}} \gets \pi_{HL}$
            \ENDIF
            \STATE \textbf{Reference Twist Field Generator}
            \STATE Interpolate action chunk $\to$ dense trajectory $T_{sd_i}(t)$
            \STATE Compute desired body twists $\mathcal{V}_i^{\text{des}}(t)$ via Eq.~\ref{eq:body_twist}
            \STATE Apply stable vector field $\to$ reference twists $\mathcal{V}_i^{\text{ref}}$ via Eq.~\ref{eq:vector_field}
            \STATE Decompose $\mathcal{V}_i^{\text{ref}} \to$ internal motion $\mathcal{V}_{i,\parallel}^{\text{ref}}$ and bulk motion $\mathcal{V}_{i,\perp}^{\text{ref}}$
            \STATE \textbf{Low-Level Policy}
            \STATE Observe $o_t = (\mathcal{V}_i^{\text{ref}}, \mathcal{B}_i, \mathcal{F}_i, T_{sb_i}, \mathcal{V}_i)$
            \STATE Sample action $a_t = (d_{i,\parallel}, d_{i,\perp}, k_{p_i}, \alpha) \sim \pi_\theta(\cdot|o_t)$
            \STATE \textbf{Controller}
            \STATE Compute commanded wrench $\mathcal{F}_{\mathrm{cmd}, i}$ via Eq.~\ref{eq:impedance_control_main}
            \STATE Execute joint torque $\tau_{\mathrm{cmd}, i} = J_i(\theta_i)^T \mathcal{F}_{\mathrm{cmd}, i}$ via Eq.~\ref{eq:joint_torque}
            \STATE \textbf{Collect experience}
            \STATE Observe $o_{t+1}$, compute reward $r_t$ via Eq.~\ref{eq:reward}
            \STATE Store transition $(o_t, a_t, r_t, o_{t+1})$
        \ENDFOR
    \STATE \textbf{PPO Update}
    \STATE Compute advantages via GAE (Eq.~\ref{eq:gae})
    \FOR{epoch = 1 to 10}
        \STATE Sample mini-batches from buffer
        \STATE Update $\theta$ via Eq.~\ref{eq:ppo}
        \STATE Update $\phi$ via MSE loss on value targets
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
