% Method
\section{Method}
\label{sec:method}

We present \textbf{SWIVL}, a hierarchical control framework that bridges high-level cognitive planning with physically grounded bimanual execution. SWIVL consists of three key components: (1) a Stable Imitation Vector Field based \textbf{Reference Motion Field Generator} that transforms discrete high-level waypoints into dense, continuous vector fields defined over the entire task space---providing stable reference motions even when the system deviates from the desired trajectory,  (2) Screw Axes Decomposition based \textbf{Twist-driven Impedance Controller} that enables practical impedance control with pose error and twist error, and (3) a Reinforcement Learning based \textbf{Wrench-feedback and Object-conditioned Impedance Variable Learning Policy} that modulates the reference motions in a physically feasible manner by explicitly incorporating object information and end-effector wrench feedback.

\textbf{Notation.} We use the following frame conventions throughout:
\begin{itemize}
    \item $\{s\}$: Spatial (world) frame
    \item $\{b_i\}$: Body frame of end-effector $i \in \{l, r\}$
    \item $\{d_i\}$: Desired frame of end-effector $i$
    \item $T_{ab}$: Transformation from frame $\{b\}$ to frame $\{a\}$
    \item ${}^a\mathcal{V}_b$: Twist of frame $\{b\}$ expressed in frame $\{a\}$
\end{itemize}

\subsection{Problem Formulation}
\label{sec:problem_formulation}

We address the problem of \textbf{bimanual manipulation of articulated objects} where two robot arms cooperatively manipulate a shared articulated object (e.g., rotating pan-tilt camera, pumping with an air pump). Our goal is to develop SWIVL, a control framework that tracks high-level motion plans while ensuring physical feasibility, stable grasping, and damage prevention during bimanual interaction. 

This problem presents four core challenges that directly motivate SWIVL's architectural components: (1) \textbf{generating dense, closed-loop references} from sparse high-level waypoints, (2) \textbf{satisfying kinematic constraints} imposed by the object's joint structure, (3) \textbf{decomposing and modulating compliance} for bulk versus internal motions on the SE(3) manifold, and (4) \textbf{regulating inter-arm forces} to minimize harmful internal wrenches. These challenges are addressed respectively by SWIVL's Reference Motion Field Generator (Section 3.2.2), Screw-decomposed Controller structure (Section 3.2.4), learned impedance variables (Section 3.2.3), and wrench-based reward design (Section 3.3.2).

\textbf{Framework Scope.} We develop the theoretical framework in \textbf{SE(3)} for generality and real-world applicability, while experimental validation (Section~\ref{sec:experiments}) is conducted in \textbf{SE(2)} planar environments to isolate core challenges of force coupling and constraint satisfaction. The formulation targets the Franka FR3 dual-arm platform for future real-world deployment and focuses on objects with revolute and prismatic joints, which constitute the majority of practical articulated object manipulation tasks.

\subsubsection{Challenge 1: From Sparse Waypoints to Dense, Closed-Loop Control}

Modern high-level policies---including vision-language-action (VLA) models, behavior cloning architectures, and teleoperation interfaces---predominantly output actions in the form of action chunks: sequences of waypoints that specify desired end-effector trajectories over a finite horizon. Regardless of the internal mechanisms of these diverse planners, SWIVL operates on a unified interface that accepts action chunks as input.

\textbf{Action Chunk Representation.} We assume that the high-level policy outputs discrete end-effector pose sequences for both arms:

\begin{equation}
\mathcal{T}_{\text{des}} = \lbrace (T_{sd_l}[\tau], T_{sd_r}[\tau]) \rbrace_{\tau=0}^{H},
\end{equation}

where $T_{sd_l}[\tau], T_{sd_r}[\tau] \in SE(3)$ represent the desired left and right end-effector poses (from spatial frame $\{s\}$ to desired frame $\{d_i\}$) at discrete time index $\tau$, and $H$ is the action chunk horizon. This action chunking paradigm, widely adopted in diffusion policies and autoregressive generative models, enables temporal consistency and multi-step reasoning in high-level planning.

\textbf{The Challenge.} These action chunks serve as desired waypoint trajectories, but present three critical gaps for low-level execution: (1) \textbf{sparsity}---waypoints are discrete while control requires continuous dense references at high frequency, (2) \textbf{open-loop nature}---when the system deviates from the desired trajectory due to tracking errors or disturbances, fixed waypoints cannot provide corrective feedback to guide the system back, and (3) \textbf{physical infeasibility}---high-level planners often generate trajectories without explicit knowledge of object constraints or force considerations, potentially violating kinematic constraints or inducing excessive internal forces. 

\textbf{Solution Preview.} This challenge is addressed by the \textbf{Reference Motion Field Generator} (Section 3.2.2), which performs SE(3) trajectory smoothing, computes body twists, and constructs a stable imitation vector field that provides dense, continuous reference motions with corrective feedback capabilities.

\subsubsection{Challenge 2: Kinematic Constraints in Bimanual Manipulation}

When two arms grasp and manipulate an articulated object, their motions are no longer independent but coupled through the object's kinematic structure. This coupling fundamentally constrains what motions are physically feasible and must be explicitly modeled for successful bimanual control.

\textbf{System Configuration.} We consider a dual-arm system manipulating articulated objects with $k$ internal degrees of freedom. Let $T_{sb_l}, T_{sb_r} \in SE(3)$ denote the left and right end-effector poses (from spatial frame $\{s\}$ to body frame $\{b_i\}$) in the spatial (world) frame, and $\mathbf{q}_{\text{obj}} \in \mathbb{R}^k$ represent the object's internal joint configuration vector (e.g., multi-link scissor angles, cabinet with multiple drawers, or complex mechanisms with multiple revolute and prismatic joints).

\textbf{Spatial Jacobian.} The kinematic structure of the articulated object is characterized by its \textbf{spatial Jacobian} $J_s(\mathbf{q}_{\text{obj}}) \in \mathbb{R}^{6 \times k}$, which encodes the relationship between the object's internal joint velocities and the resulting relative motion between the two grasp points. Each column of $J_s$ represents a spatial screw axis $\mathcal{S}_j \in \mathbb{R}^6$ corresponding to the $j$-th joint:

\begin{equation}
J_s(\mathbf{q}_{\text{obj}}) = \begin{bmatrix} \mathcal{S}_1(\mathbf{q}_{\text{obj}}) & \mathcal{S}_2(\mathbf{q}_{\text{obj}}) & \cdots & \mathcal{S}_k(\mathbf{q}_{\text{obj}}) \end{bmatrix},
\end{equation}

where each screw axis $\mathcal{S}_j = \begin{bmatrix} \omega_j \\ v_j \end{bmatrix}$ describes the instantaneous motion induced by unit velocity of joint $j$, expressed in the spatial frame. For revolute joints, $\mathcal{S}_j$ represents the axis of rotation and a point on the axis; for prismatic joints, it represents the direction of translation.

\textbf{Holonomic Constraint.} The object's kinematic structure imposes a holonomic constraint that couples the end-effectors' velocities. Since both end-effectors are rigidly grasping the articulated object, the \textbf{relative motion} between them must exactly match the motion generated by the object's internal joints:

\begin{equation}
{}^s\mathcal{V}_{b_l} - {}^s\mathcal{V}_{b_r} = J_s(\mathbf{q}_{\text{obj}}) \dot{\mathbf{q}}_{\text{obj}},
\end{equation}

where ${}^s\mathcal{V}_{b_l}, {}^s\mathcal{V}_{b_r} \in \mathbb{R}^6$ are the spatial twists (6D velocities) of each end-effector, and $\dot{\mathbf{q}}_{\text{obj}} \in \mathbb{R}^k$ is the object's joint velocity vector. This constraint specifies that the relative motion between the two arms must lie in the range space of $J_s$, aligning precisely with the object's allowable internal motions along its kinematic chain.

\textbf{The Challenge.} Any control policy for bimanual manipulation must ensure that commanded motions satisfy this holonomic constraint. Violating the constraint leads to physically infeasible trajectories that cause grasp slippage, internal stress accumulation, and potential task failure. 

\textbf{Solution Preview.} This challenge is addressed by the \textbf{Screw-decomposed Controller} (Section 3.2.4), which uses orthogonal projection operators based on the object Jacobian to structurally decompose commanded motions into components that lie in the range of $J_i$ (constraint-satisfying internal motion) and its orthogonal complement (bulk motion), guaranteeing kinematic feasibility by construction.

\subsubsection{Challenge 3: Decomposing and Modulating Compliance on SE(3)}

To enable task-agnostic control, the low-level policy must interpret high-level intentions directly from trajectory structure without task-specific annotations. We observe that bimanual manipulation of articulated objects inherently involves two distinct motion intentions: moving the object as a whole through space, and actuating its internal joint.

\textbf{Semantic Motion Decomposition.} For task semantics, we want to decompose each end-effector's reference motion into two semantically meaningful components:

\begin{itemize}
    \item \textbf{Bulk motion} (orthogonal to screw axis): Drives the object's \textbf{overall motion} through space---transport, reorientation, and gross positioning. The term is borrowed from fluid dynamics, where ``bulk motion'' refers to macroscopic flow of the entire fluid body.
    \item \textbf{Internal motion} (parallel to screw axis): Drives the object's \textbf{joint articulation}---opening, closing, rotating, or extending the internal degree of freedom. This represents productive motion that directly actuates the object's constrained joint.
\end{itemize}

By explicitly providing both components to the policy, we enable it to infer task semantics from trajectory structure: tasks emphasizing bulk motion (e.g., transporting a closed scissor) vs. tasks emphasizing internal motion (e.g., cutting with stationary scissors) vs. coordinated tasks (e.g., simultaneously moving and actuating). This decomposition enables generalization across diverse manipulation objectives without explicit task labels.

\textbf{Orthogonality and Metric Definition.} Performing this decomposition requires separating parallel and orthogonal components relative to the screw axis, which necessitates defining an inner product on the Lie algebra $\mathfrak{se}(3)$. However, in a purely kinematic sense, there is no natural inner product that defines orthogonality between two twists in terms of rigid body motion. This is fundamentally because purely kinematic formulations lack a natural physical scale to compare rotations (dimensionless) with translations (length), making any choice of inner product physically arbitrary. Standard candidates—Euclidean product on $\mathbb{R}^6$ (mixes incompatible units, not frame-invariant), reciprocal product (requires dual space pairing), Killing form (degenerate for SE(3))—all present difficulties for physically meaningful rigid body motion decomposition (see Appendix~\ref{app:orthogonal_decomposition} for detailed analysis).

\textbf{SE(3) Impedance Control Limitations.} Impedance control---regulating the dynamic relationship between motion and force---is a natural paradigm for compliant bimanual manipulation. However, implementing impedance control on the SE(3) manifold presents fundamental mathematical challenges. Classical Cartesian impedance control derives elastic wrenches from potential energy via differentiation, which for SE(3) introduces a nonlinear Jacobian-like operator $J_{\mathcal{E}}$ (detailed in Section 3.1.5 below). This nonlinearity fundamentally limits the design freedom of the stiffness matrix $K$: achieving desired compliance behavior in one region of the configuration space may produce unintended, potentially unstable behavior elsewhere due to the varying geometric structure.

\textbf{The Challenge.} The core challenge arises when attempting to integrate impedance control with task-semantic decomposition. For effective bimanual manipulation, we require \textbf{independent compliance modulation} along bulk versus internal motion directions. However, the nonlinear coupling in $J_{\mathcal{E}}$ prevents straightforward decomposition-aware stiffness design. Additionally, the choice of metric for orthogonal decomposition directly affects which motions are considered "parallel" or "orthogonal" to the constraint, yet no single choice is universally optimal across all tasks and object geometries.

\textbf{Solution Preview.} These challenges are addressed through three coupled mechanisms: (1) the \textbf{Learned Impedance Variables} (Section 3.2.3), where the policy outputs separate damping coefficients $d_{i,\parallel}$ and $d_{i,\perp}$ for independent compliance modulation, and critically, a \textbf{learnable characteristic length scale} $\alpha$ that adaptively defines the metric tensor $G = \mathrm{diag}(\alpha^2 I_3, I_3)$, allowing the policy to discover task-appropriate notions of orthogonality; (2) the \textbf{Screw-decomposed Controller} (Section 3.2.4), which constructs projection operators using this learned metric to decompose control actions into bulk and internal components; and (3) the \textbf{Reference Motion Field} (Section 3.2.2), which incorporates the stability term $k_{p_i} \mathcal{E}_i$ directly into the reference twist, sidestepping the explicit nonlinear Jacobian $J_{\mathcal{E}}$ while maintaining SE(3) impedance behavior.

\subsubsection{Challenge 4: Force Coupling and Internal Wrenches}

In bimanual manipulation, when both arms rigidly grasp the same object, their force interactions become coupled. Any coordination error that violates the kinematic constraint generates \textbf{internal wrenches}---forces and torques that stress the object and grasps without contributing to productive manipulation.

\textbf{Internal Wrenches.} Let $\mathcal{F}_l, \mathcal{F}_r \in \mathbb{R}^6$ denote body wrenches measured at the end-effectors: $\mathcal{F}_i = \begin{bmatrix} m_i \\ f_i \end{bmatrix}$ (moment and force). When the two arms' motions deviate from the kinematic constraint (Eq.~3), they ``fight'' each other, generating internal wrenches that: (1) increase contact stress and risk grasp failure, (2) waste energy through non-productive internal loading, and (3) potentially damage the object or robot hardware. 

\textbf{The Challenge.} For compliant manipulation, we must distinguish between force components that contribute to desired object motion versus those that only create internal stress. The control framework needs to identify and actively minimize non-productive internal forces while maintaining necessary productive forces for manipulation. This requires decomposing measured wrenches into components aligned with (productive) and orthogonal to (internal) the object's allowable motion direction. 

\textbf{Solution Preview.} This challenge is addressed through the \textbf{Wrench-based Reward Design} (Section 3.3.2), which decomposes measured wrenches using the transpose of twist projection operators $\mathcal{F}_{i,\perp} = P_{i,\perp}^T \mathcal{F}_i$ to identify internal wrenches orthogonal to the object's allowable motion, and explicitly penalizes $\|\mathcal{F}_{i,\perp}\|_2^2$ to train the policy to minimize non-productive forces. This wrench decomposition is fully consistent with the twist decomposition framework, exploiting the duality between twist and wrench spaces under the reciprocal product (virtual power).

\subsubsection{Mathematical Background: SE(3) Impedance Control Geometry}

For completeness, we provide the geometric details underlying Challenge 3. Impedance control---regulating the dynamic relationship between motion and force---is a natural paradigm for compliant bimanual manipulation. However, implementing impedance control on the SE(3) manifold presents fundamental mathematical challenges that motivate our learning-based approach.

\textbf{Kinetic Energy Metric and Distance Problem.} Unlike Euclidean spaces, SE(3) is a manifold where defining meaningful error metrics is non-trivial. The most physically natural metric on SE(3) is the \textbf{kinetic energy-based Riemannian metric} induced by the spatial inertia matrix $M \in \mathbb{R}^{6 \times 6}$. For tracking control, we need to measure the "distance" between the current pose $T$ and desired pose $T_{\text{des}}$, typically represented by the error pose $T_{\text{err}} = T_{\text{des}}^{-1} T$. However, SE(3) lacks a \textbf{bi-invariant Riemannian metric}---a metric that remains consistent under both left and right group actions. The exponential map follows screw motions (constant-twist trajectories), while geodesics under the kinetic energy metric follow more complex, inertia-weighted paths. This mismatch means that the natural logarithm map $\log(T_{\text{err}})^\vee \in \mathfrak{se}(3)$, commonly used to define pose errors, does not correspond to minimal-energy paths under the physically meaningful metric.

\textbf{Nonlinear Stiffness and Design Constraints.} In classical Cartesian impedance control, elastic wrenches are derived from potential energy via differentiation. For SE(3), a nonlinear Jacobian-like operator emerges:

\begin{equation}
{\mathcal{F}_{\mathrm{elastic}} = -J_{\mathcal{E}}^\top K \mathcal{E}} \, ,
\end{equation}

where $J_{\mathcal{E}}$ is given by:
\begin{equation}
J_{\mathcal{E}} = \begin{pmatrix} \alpha J_l^{-1}(e_R) & 0_{3 \times 3} \\ -[e_p] & I_{3} \end{pmatrix} \in \mathbb{R}^{6 \times 6} \, .
\end{equation}
Here, $J_l(\theta)$ is the left Jacobian of SO(3):
\begin{equation}
J_l(\theta) = I + \frac{1 - \cos \|\theta\|}{\|\theta\|^2} [\theta] + \frac{\|\theta\| - \sin \|\theta\|}{\|\theta\|^3} [\theta]^2 \, .
\end{equation} 
$J_{\mathcal{E}}$ is configuration-dependent and couples rotational and translational components in complex, state-dependent ways. This nonlinearity fundamentally limits the design freedom of the stiffness matrix $K$: achieving desired compliance behavior in one region of the configuration space may produce unintended, potentially unstable behavior elsewhere due to the varying geometric structure. These mathematical constraints motivate our learning-based approach described in Challenge 3 above.

\subsection{SWIVL Architecture}

SWIVL adopts a \textbf{four-layer hierarchical architecture} that directly addresses the four challenges identified above. The architecture decouples high-level reasoning from low-level physical interaction:

\begin{itemize}
    \item \textbf{Layer 1 (High-Level Policy)}: Provides task-level guidance through sparse waypoint generation
    \item \textbf{Layer 2 (Reference Motion Field Generator)}: Addresses Challenge 1 by transforming sparse waypoints into dense, closed-loop reference trajectories
    \item \textbf{Layer 3 (Impedance Variable Modulation Policy)}: Addresses Challenges 3 \& 4 by learning adaptive compliance modulation based on object geometry and wrench feedback
    \item \textbf{Layer 4 (Screw-decomposed Controller)}: Addresses Challenges 2 \& 3 by structurally enforcing kinematic constraints through orthogonal motion decomposition
\end{itemize}

The core innovation lies in the tight integration of Layers 2-4, which together enable physically grounded, force-compliant bimanual manipulation underneath arbitrary high-level planners.

\subsubsection{Layer 1: High-Level Policy}

The top layer generates goal-directed behavior in action chunk. This layer can be instantiated by:
\begin{itemize}
    \item \textbf{Vision-Language-Action (VLA) models}
    \item \textbf{Behavior cloning policies}
    \item \textbf{Teleoperation interfaces}
\end{itemize}

\textbf{Interface:} Outputs discrete end-effector pose waypoints $\lbrace T_{sd_l}[\tau], T_{sd_r}[\tau] \rbrace_{\tau=0}^{H}$ at low frequency.

\subsubsection{Layer 2: Reference Motion Field Generator}

This layer addresses Challenge 1 by bridging the gap between discrete high-level waypoints and continuous low-level control. High-level policies provide sparse waypoints at low frequency ($\sim$10Hz), while the low-level controller requires smooth, dense reference trajectories at high frequency ($\sim$50Hz) with corrective feedback capabilities. The Reference Motion Field Generator performs three steps to achieve this transformation:

\paragraph{Step 1: SE(3) Trajectory Smoothing.} To address the sparsity gap, we perform smooth interpolation in $SE(3)$ to obtain dense desired trajectories at the Low-Level Policy frequency $\Delta t_{LL} \ll \Delta t_{HL}$:

\begin{equation}
\lbrace T_{sd_l}(t), T_{sd_r}(t) \rbrace_{t=0}^{H_{LL}},
\end{equation}

where $H_{LL}$ is the smoothed trajectory horizon. We use SLERP for rotations and cubic splines for translations; see Appendix~\ref{app:trajectory_interpolation} for details. Both interpolation schemes are differentiable in time, so they induce smooth position and rotation trajectories $p_i^{\text{des}}(t)$ and $R_i^{\text{des}}(t)$ for each end-effector.

\paragraph{Step 2: Body Twist Computation.} For a smooth trajectory $T_{sd_i}(t) = \begin{bmatrix} R_{sd_i}(t) & p_{sd_i}(t) \\ 0 & 1 \end{bmatrix}$, we directly compute the desired twist by differentiating the pose and expressing the resulting velocities in the instantaneous desired frame. Let $R_{sd_i}(t)$ denote the rotation from the desired body frame $\{d_i\}$ to the spatial frame $\{s\}$, and $p_{sd_i}(t)$ the position of the desired body frame origin in the spatial frame. The corresponding desired-frame angular and linear velocities are given by

\begin{equation}
\label{eq:body_twist}
\mathcal{V}_i^{\text{des}}(t) =
\begin{bmatrix}
\omega_i^{\text{des}}(t) \\
v_i^{\text{des}}(t)
\end{bmatrix},
\quad [\omega_i^{\text{des}}(t)]_\times = R_{sd_i}(t)^{\top} \dot{R}_{sd_i}(t),
\quad v_i^{\text{des}}(t) = R_{sd_i}(t)^{\top} \dot{p}_{sd_i}(t).
\end{equation}

Here $[\omega]_\times$ is the skew-symmetric matrix representation of the angular velocity vector $\omega$, and both $\omega_i^{\text{des}}(t)$ and $v_i^{\text{des}}(t)$ are expressed in the desired frame $\{d_i\}$ by construction. 

\paragraph{Step 3: Stable Imitation Vector Field Design.} 
As execution progresses within an action chunk, the actual end-effector poses may deviate significantly from the desired trajectory due to tracking errors, disturbances, and model mismatch. 
To address the open-loop nature of waypoints and provide robustness when the system deviates from the desired trajectory, we construct a vector field that balances \textbf{imitation} of demonstrated motions and \textbf{stability} for error correction.
We employ a stable imitation vector field that combines two components: an imitation term that mimics the demonstrated velocity profile at the temporally synchronized point, and a stability term that provides corrective feedback via a term proportional to the SE(3) pose error for convergence to the desired trajectory:

\begin{equation}
\label{eq:vector_field}
\mathcal{V}_i^{\text{ref}}(t, T_{sb_i}) = \mathrm{Ad}_{T_{b_id_i}}\mathcal{V}_i^{\text{des}}(t) + k_{p_i} \mathcal{E},
\end{equation}
where $\mathrm{Ad}_T$ denotes the adjoint transformation that maps twists between frames. Since the desired twist $\mathcal{V}_i^{\text{des}}(t)$ is computed in the desired frame $\{d_i\}$ (Eq.~\ref{eq:body_twist}), we must transform it to the current body frame $\{b_i\}$ where the controller operates. The transformation $T_{b_id_i} = T_{b_is} T_{sd_i} = (T_{sb_i})^{-1} T_{sd_i}$ represents the relative transformation from the desired frame to the current body frame, and $\mathrm{Ad}_{T_{b_id_i}}$ performs the corresponding twist transformation. The pose error term is given by:
\begin{equation}
\begin{aligned}
\mathcal{E} &= \begin{pmatrix} \alpha e_R \\ e_p \end{pmatrix} \in \mathbb{R}^6 \\
e_{R_i} &= \log(R_{sb_i}^\top R_{sd_i})^\vee \in \mathbb{R}^3 \quad \text{(rotation error in body frame)} \\
e_{p_i} &= R_{sb_i}^\top(p_{sd_i} - p_{sb_i}) \in \mathbb{R}^3 \quad \text{(translation error in body frame)} \\
k_{p_i} &\in \mathbb{R} \quad \text{(scalar proportional gain)} \\
\end{aligned}
\end{equation}
Here, $\alpha$ represents a characteristic length that weights the rotational cost relative to translation, where $G = \mathrm{diag}(\alpha^2 I_3, I_3)$ is the metric tensor defining an inner product $\langle \cdot, \cdot \rangle_G$ on $\mathfrak{se}(3)$:
\begin{equation}
\langle \mathcal{V}_1, \mathcal{V}_2 \rangle_G = \frac{\alpha^2}{2} \mathrm{tr}([\omega_1]^\top [\omega_2]) + v_1^\top v_2 =  \mathcal{V}_1^\top G \mathcal{V}_2 \, ,
\end{equation}
where $\mathcal{V}_i = \begin{bmatrix} \omega_i \\ v_i \end{bmatrix}$.
This time-based approach offers computational efficiency ($O(1)$ lookup) while maintaining strong tracking performance for smooth bimanual trajectories. The term $k_p \mathcal{E}$, as will be described later, plays a role in creating an elastic force for impedance control without using the nonlinear Jacobian matrix to create a reference twist.

\subsubsection{Layer 3: Impedance Variable Modulation Policy}

This layer addresses Challenges 3 and 4 by learning adaptive impedance modulation. The Reinforcement Learning Policy $\pi_\theta: \mathcal{O} \to \mathcal{A}$ modulates impedance variables to enable physically feasible motions while accounting for object constraints and inter-arm force interactions. By explicitly conditioning on object geometry (screw axes) and wrench feedback, the policy learns to independently modulate compliance for bulk versus internal motions and minimize harmful internal forces.

\textbf{Observation Space $\mathcal{O}$.} The policy receives:

\begin{enumerate}
    \item \textbf{Reference twists}: $\lbrace \mathcal{V}_l^{\text{ref}}, \mathcal{V}_r^{\text{ref}} \rbrace \in \mathfrak{se}(3) \times \mathfrak{se}(3)$
    These are the reference motions computed by the Reference Motion Field Generator (Layer 2) at the current time $t$ and current end-effector poses $T_{sb_l}, T_{sb_r}$.

    \item \textbf{Object constraints}: $\lbrace \mathcal{B}_l, \mathcal{B}_r \rbrace \in \mathfrak{se}(3) \times \mathfrak{se}(3)$
    These encode the kinematic constraint of the manipulated object. $\mathcal{B}_l, \mathcal{B}_r$ are the body-frame screw axes defining the object's allowable internal motion directions at each end-effector. For articulated objects, these correspond to the columns of the object Jacobian $J_i$, related to the spatial screw axis via $\mathcal{B}_i = \mathrm{Ad}_{T_{b_is}} \mathcal{S}$ where $\mathrm{Ad}_{T_{b_is}}$ transforms spatial twists to body frame.

    \item \textbf{Wrench feedback}: $\lbrace \mathcal{F}_l, \mathcal{F}_r \rbrace \in \mathfrak{se}(3)^* \times \mathfrak{se}(3)^*$
    These are 6-dimensional wrench measurements (3D moment + 3D force) obtained from 6-axis force-torque sensors mounted at each end-effector's wrist. Raw sensor data may be filtered (e.g., low-pass filtering or exponential smoothing) when necessary to reduce measurement noise while preserving force feedback responsiveness for compliant control.

    \item \textbf{Proprioception}: $\lbrace T_{sb_l}, T_{sb_r}, \mathcal{V}_l, \mathcal{V}_r \rbrace$
    Task-space states including end-effector poses $T_{sb_l}, T_{sb_r} \in \mathrm{SE}(3)$ and body twists $\mathcal{V}_l, \mathcal{V}_r \in \mathfrak{se}(3)$.
\end{enumerate}

\textbf{Action Space $\mathcal{A}$.} We propose an impedance variable action space that structurally enforces object's kinematic constraints by motion decomposition. These variables parameterize the low-level controller (detailed in Section~\ref{sec:layer4_controller}), allowing the policy to modulate compliance behavior dynamically:
\begin{equation}
a_t = ( d_{l,\parallel},  d_{r,\parallel}, d_{l,\perp},  d_{r,\perp},k_{p_l},  k_{p_r},\alpha) \in \mathbb{R}^7,
\end{equation}
where $d_{i,\parallel}, d_{i,\perp}, k_{p_i}, \alpha \in \mathbb{R}^+$ are positive scalar gains, and:
\begin{itemize}
    \item $ d_{i,\parallel}$: Damping coefficient for internal motion (parallel to screw axis), regulating compliance along the object's degree of freedom.
    \item $ d_{i,\perp}$: Damping coefficient for bulk motion (orthogonal to screw axis), regulating compliance for the object's overall transport.
    \item $k_{p_i}$: Stiffness gain for the stability term in the vector field, determining the strength of correction towards the desired trajectory.
    \item $\alpha$: \textbf{Learnable characteristic length scale} that adaptively weights rotational error relative to translational error in the SE(3) metric tensor $G = \mathrm{diag}(\alpha^2 I_3, I_3)$. By learning $\alpha$, the policy discovers task-appropriate metric structures for orthogonal decomposition and compliance modulation.
\end{itemize}

These policy outputs parameterize the low-level controller (detailed in Layer 4 below), enabling adaptive, context-dependent impedance modulation.

\subsubsection{Layer 4: Screw-decomposed Twist-driven Impedance Controller}
\label{sec:layer4_controller}

This layer addresses Challenges 2 and 3 by executing low-level control that structurally enforces kinematic constraints while enabling independent compliance modulation. It tracks the reference motion $\mathcal{V}_i^{\text{ref}}$ from Layer 2 using the impedance parameters from Layer 3. The key innovation of this controller is its ability to provide \textbf{two complementary views}: (1) it behaves as a geometrically consistent SE(3) impedance controller, and (2) it explicitly decomposes control actions into bulk and internal motion spaces, enabling task-aware compliance modulation.

\textbf{Orthogonal Decomposition via Object Jacobian.} To enable motion decomposition, we first establish projection operators that separate motions into components parallel and orthogonal to the object's kinematic constraints. Let $J_i(\mathbf{q}_{\text{obj}}) \in \mathbb{R}^{6 \times k}$ denote the body Jacobian of the object for end-effector $i \in \{l, r\}$, which encodes how the object's joint velocities $\dot{\mathbf{q}}_{\text{obj}}$ manifest as end-effector body twists. The body Jacobian relates to the spatial Jacobian (Eq.~2) via the adjoint transformation: $J_i = \mathrm{Ad}_{T_{b_is}} J_s$, where $T_{b_is} = (T_{sb_i})^{-1}$ transforms spatial frame quantities to the body frame.

Using the inner product $\langle \mathcal{V}_1, \mathcal{V}_2 \rangle_G = \mathcal{V}_1^T G \mathcal{V}_2$ with metric tensor $G = \mathrm{diag}(\alpha^2 I_3, I_3)$ on $\mathfrak{se}(3)$, we construct orthogonal projection operators:

\begin{equation}
\label{eq:projection_operators}
\begin{aligned}
P_{i,\parallel} &= J_i (J_i^\top G J_i)^{-1} J_i^\top G, \quad P_{i,\perp} = I - P_{i,\parallel},
\end{aligned}
\end{equation}

where $P_{i,\parallel}$ projects onto the internal motion subspace (range of $J_i$) and $P_{i,\perp}$ projects onto the bulk motion subspace (orthogonal complement). These projectors satisfy $P_{i,\parallel}^T G = G P_{i,\parallel}$ and $P_{i,\perp}^T G = G P_{i,\perp}$, ensuring geometric consistency under the chosen metric. Importantly, since $\alpha$ is learned by the policy (Layer 3), the metric tensor $G$ and hence the projection operators adapt dynamically to task requirements, enabling context-dependent orthogonal decomposition.

\textbf{Controller Formulation.} With the learned impedance variables $a_t = (d_{l,\parallel}, d_{r,\parallel}, d_{l,\perp}, d_{r,\perp}, k_{p_l}, k_{p_r}, \alpha)$ from Layer 3, we construct a damping matrix that respects the motion decomposition:

\begin{equation}
\label{eq:damping_matrix}
K_{d_i} = G (P_{i,\parallel} d_{i,\parallel} + P_{i,\perp} d_{i,\perp}),
\end{equation}

This structure allows independent damping modulation for internal motion (via $d_{i,\parallel}$) and bulk motion (via $d_{i,\perp}$), enabling the policy to adaptively regulate compliance based on task requirements and force feedback.

The commanded wrench is then computed as:

\begin{equation}
\label{eq:impedance_control_main}
\begin{aligned}
\mathcal{F}_{\mathrm{cmd}, i} &= K_{d_i} (\mathcal{V}_i^{\text{ref}} - \mathcal{V}_i) + \mu_{b,i} + \gamma_{b,i}\\
&= K_{d_i} (\mathrm{Ad}_{T_{b_id_i}} \mathcal{V}_i^{\text{des}} - \mathcal{V}_i + k_{p_i} \mathcal{E}_i) + \mu_{b,i} + \gamma_{b,i},
\end{aligned}
\end{equation}

where $\mathcal{V}_i^{\text{ref}} = \mathrm{Ad}_{T_{b_id_i}} \mathcal{V}_i^{\text{des}} + k_{p_i} \mathcal{E}_i$ is the reference twist from Layer 2, $\mu_{b,i}$ accounts for nonlinear dynamics (Coriolis and centrifugal terms), and $\gamma_{b,i}$ provides gravity compensation (omitted in planar SE(2) settings where gravity is orthogonal to the motion plane). 

The commanded wrench is then mapped to joint-space motor torques via the manipulator Jacobian:

\begin{equation}
\label{eq:joint_torque}
\tau_{\mathrm{cmd}, i} = J_i(\theta_i)^T \mathcal{F}_{\mathrm{cmd}, i},
\end{equation}

where $J_i(\theta_i) \in \mathbb{R}^{6 \times n}$ is the geometric Jacobian of the $i$-th manipulator mapping joint velocities to end-effector twist, and $\tau_{\mathrm{cmd}, i} \in \mathbb{R}^n$ is the commanded joint torque vector that serves as the final robot control command.

We now provide two complementary interpretations that reveal the dual nature of this controller.

\textbf{Interpretation 1: SE(3) Impedance Control Structure.} 

The first interpretation reveals that our controller naturally implements SE(3) impedance control. Classical impedance control on SE(3) designs a virtual dynamical system with desired impedance characteristics:

\begin{equation}
M \dot{\xi} + D \xi + J_{\mathcal{E}}^\top K \mathcal{E} = \mathcal{F}_{\mathrm{ext}},
\end{equation}

where $\xi = {}^b\mathcal{V}_d - {}^b\mathcal{V}_b$ is the velocity error, $M$ is the desired inertia, $D$ is damping, and $J_{\mathcal{E}}^\top K \mathcal{E}$ is the nonlinear stiffness term arising from SE(3) geometry (Section~\ref{sec:problem_formulation}). The corresponding impedance controller takes the form:

\begin{equation}
\mathcal{F}_{\mathrm{cmd}} = \Lambda_b M^{-1}(D \xi + J_{\mathcal{E}}^\top K \mathcal{E}) + \Lambda_b {}^b\dot{\mathcal{V}}_d + \mu_b + \gamma_b + (I - \Lambda_b M^{-1})\mathcal{F}_{\mathrm{ext}},
\end{equation}

where $\Lambda_b$ is the operational space inertia matrix. Under the common simplifications $M = \Lambda_b$ (match desired and actual inertia) and ${}^b\dot{\mathcal{V}}_d = 0$ (constant reference velocity), this reduces to:

\begin{equation}
\mathcal{F}_{\mathrm{cmd}} = D \xi + J_{\mathcal{E}}^\top K \mathcal{E} + \mu_{b} + \gamma_{b}.
\end{equation}

Our controller in Eq.~\eqref{eq:impedance_control_main} follows this exact structure. Defining the velocity error as $\xi = \mathrm{Ad}_{T_{b_id_i}} \mathcal{V}_i^{\text{des}} - \mathcal{V}_i$, we can rewrite:

\begin{equation}
\label{eq:impedance_equivalence}
\begin{aligned}
\mathcal{F}_{\mathrm{cmd}, i} &= K_{d_i} (\mathrm{Ad}_{T_{b_id_i}} \mathcal{V}_i^{\text{des}} - \mathcal{V}_i + k_{p_i} \mathcal{E}_i) + \mu_{b,i} + \gamma_{b,i}\\
&= K_{d_i} \xi + K_{d_i} k_{p_i} \mathcal{E}_i + \mu_{b,i} + \gamma_{b,i}\\
&\approx D \xi + J_{\mathcal{E}}^\top K \mathcal{E} + \mu_{b} + \gamma_{b},
\end{aligned}
\end{equation}

where the correspondence is: $D \leftrightarrow K_{d_i}$ (learned damping) and the term $K_{d_i} k_{p_i} \mathcal{E}_i$ plays the role of $J_{\mathcal{E}}^\top K \mathcal{E}$ (stiffness). Critically, our approach \textbf{sidesteps the explicit nonlinear Jacobian $J_{\mathcal{E}}$} by incorporating the $k_{p_i} \mathcal{E}_i$ term directly into the reference twist (Layer 2), avoiding the geometric complications discussed in Section~\ref{sec:problem_formulation} while maintaining impedance behavior.

\textbf{Interpretation 2: Explicit Bulk-Internal Motion Decomposition.} 

The second interpretation reveals how the controller naturally decomposes control actions into semantically meaningful components aligned with task requirements. By decomposing both reference and actual twists into components parallel (internal) and orthogonal (bulk) to the object's kinematic constraints:

\begin{align}
\mathcal{V}_{i,\parallel}^{\text{ref}} &= P_{i,\parallel} \mathcal{V}_i^{\text{ref}}, \quad \mathcal{V}_{i,\parallel} = P_{i,\parallel} \mathcal{V}_i \quad \text{(internal motion)}, \\
\mathcal{V}_{i,\perp}^{\text{ref}} &= P_{i,\perp} \mathcal{V}_i^{\text{ref}}, \quad \mathcal{V}_{i,\perp} = P_{i,\perp} \mathcal{V}_i \quad \text{(bulk motion)},
\end{align}

we can expand the control law to reveal independent regulation of each motion component:

\begin{equation}
\label{eq:decomposed_control}
\begin{aligned}
\mathcal{F}_{\mathrm{cmd}, i} &= K_{d_i} (\mathcal{V}_i^{\text{ref}} - \mathcal{V}_i) + \mu_{b,i} + \gamma_{b,i}\\
&= G(P_{i,\parallel} d_{i,\parallel} + P_{i,\perp} d_{i,\perp})(\mathcal{V}_i^{\text{ref}} - \mathcal{V}_i) + \mu_{b,i} + \gamma_{b,i}\\
&= \underbrace{d_{i,\parallel} G (\mathcal{V}_{i,\parallel}^{\text{ref}} - \mathcal{V}_{i,\parallel})}_{\text{internal motion control}} + \underbrace{d_{i,\perp} G (\mathcal{V}_{i,\perp}^{\text{ref}} - \mathcal{V}_{i,\perp})}_{\text{bulk motion control}} + \mu_{b,i} + \gamma_{b,i}.
\end{aligned}
\end{equation}

This decomposition provides three critical properties:

\begin{enumerate}
    \item \textbf{Independent compliance modulation}: The policy can independently adjust $d_{i,\parallel}$ and $d_{i,\perp}$ to achieve task-specific compliance---high stiffness for bulk motion during transport, high compliance for internal motion during articulation, or vice versa.
    
    \item \textbf{Decoupled Power Generation}: The feedback wrenches for internal and bulk motions are \textbf{reciprocally orthogonal} to the opposing motion subspaces, ensuring zero interference in terms of virtual power:
    \begin{align}
 \mathcal{F}_{\mathrm{cmd,fb}, i,\parallel} &= d_{i,\parallel}G (\mathcal{V}_{i,\parallel}^{\text{ref}}-\mathcal{V}_{i,\parallel}) \\
\mathcal{F}_{\mathrm{cmd,fb}, i,\perp} &= d_{i,\perp} G (\mathcal{V}_{i,\perp}^{\text{ref}}-\mathcal{V}_{i,\perp})\\
(\mathcal{F}_{\mathrm{cmd,fb}, i,\parallel} )^\top (\mathcal{V}_{i,\perp}^{\text{ref}}-\mathcal{V}_{i,\perp}) &= 0\\
(\mathcal{F}_{\mathrm{cmd,fb}, i,\perp} )^\top (\mathcal{V}_{i,\parallel}^{\text{ref}}-\mathcal{V}_{i,\parallel}) &= 0 \, .
    \end{align}
    This orthogonality follows directly from the projection properties: $P_{i,\parallel}^T G P_{i,\perp} = 0$, ensuring that control actions for each motion type do not interfere with each other.
    
    \item \textbf{Constraint satisfaction}: The internal motion component $\mathcal{V}_{i,\parallel}$ automatically lies in the range of $J_i$, ensuring that commanded motions respect the object's kinematic constraints and minimize harmful internal forces.
\end{enumerate}

Together, these two interpretations demonstrate that our controller simultaneously achieves geometrically consistent SE(3) impedance behavior while enabling explicit, learning-based modulation of task-semantic motion components---a capability that would be intractable to design analytically given the geometric constraints discussed in Section~\ref{sec:problem_formulation}.


\subsection{Learning Framework}

\subsubsection{Reinforcement Learning Formulation}

We formulate the Low-Level Policy learning as a Partially Observable Markov Decision Process (POMDP) $\mathcal{M} = (\mathcal{S}, \mathcal{O}, \mathcal{A}, P, r, \gamma)$, where:
\begin{itemize}
    \item $\mathcal{S}$: State space (full environment state)
    \item $\mathcal{O}$: Observation space (partial observations available to the policy)
    \item $\mathcal{A}$: Action space
    \item $P(s_{t+1} | s_t, a_t)$: Transition dynamics
    \item $r: \mathcal{S} \times \mathcal{A} \to \mathbb{R}$: Reward function (Section~\ref{sec:reward_design})
    \item $\gamma$: Discount factor
\end{itemize}

The objective is to maximize expected return:

\begin{equation}
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{T} \gamma^t r(s_t, a_t) \right].
\end{equation}

\subsubsection{Reward Function Design}
\label{sec:reward_design}

The reward function balances three objectives for stable, force-compliant manipulation:

\begin{equation}
\label{eq:reward}
r_t = r_{\text{track}} + r_{\text{safety}} + r_{\text{reg}}.
\end{equation}

\textbf{Motion Tracking ($r_{\text{track}}$).} Ensures accurate tracking of reference trajectories generated by the motion field:

\begin{align}
\label{eq:reward_tracking}
r_{\text{track}} = -w_{\text{track}}\sum_{i \in \lbrace l,r \rbrace} \|\mathcal{V}_i - \mathcal{V}_i^{\text{ref}}\|_G^2 = -w_{\text{track}}\sum_{i \in \lbrace l,r \rbrace} (\mathcal{V}_i - \mathcal{V}_i^{\text{ref}})^T G (\mathcal{V}_i - \mathcal{V}_i^{\text{ref}}),
\end{align}

where $G = \mathrm{diag}(\alpha^2 I_3, I_3)$ is the learned metric tensor. This reward encourages each end-effector to follow its reference twist, where $\mathcal{V}_l, \mathcal{V}_r$ are the actual body twists and $\mathcal{V}_l^{\text{ref}}, \mathcal{V}_r^{\text{ref}}$ are the reference twists from the motion field. Using the G-metric ensures that tracking error is measured consistently with the impedance control framework, with adaptive weighting between rotational and translational components via the learned parameter $\alpha$.

\textbf{Safety ($r_{\text{safety}}$).} Ensures safe operation and minimizes non-productive internal forces:

\begin{align}
\label{eq:reward_safety}
r_{\text{safety}} = -w_{\text{int}} \sum_{i \in \lbrace l,r \rbrace} \|\mathcal{F}_{i,\perp}\|_2^2,
\end{align}

where $\mathcal{F}_i = \begin{bmatrix} m_i \\ f_i \end{bmatrix}$ is the measured wrench at each end-effector $i$.

This reward addresses the internal wrench problem identified in Section~\ref{sec:problem_formulation} by minimizing internal wrenches---wrench components orthogonal to the object's allowable motion direction. To decompose measured wrenches consistently with the twist decomposition framework in Layer 4, we seek wrench components $\mathcal{F}_{i,\parallel}$ and $\mathcal{F}_{i,\perp}$ such that they are orthogonal to complementary twist subspaces under the reciprocal product (virtual power). Specifically, we require:

\begin{align}
\langle \mathcal{F}_{i,\parallel}, \mathcal{V} \rangle &= \mathcal{F}_{i,\parallel}^T \mathcal{V} = 0 \quad \forall \mathcal{V} \in \text{range}(P_{i,\perp}), \\
\langle \mathcal{F}_{i,\perp}, \mathcal{V} \rangle &= \mathcal{F}_{i,\perp}^T \mathcal{V} = 0 \quad \forall \mathcal{V} \in \text{range}(P_{i,\parallel}),
\end{align}

where $\langle \mathcal{F}, \mathcal{V} \rangle = \mathcal{F}^T \mathcal{V}$ is the reciprocal product representing virtual power. This orthogonality condition is naturally satisfied by projecting the measured wrench using the transpose of the twist projection operators, exploiting the dual relationship between twist and wrench spaces:

\begin{equation}
\mathcal{F}_{i,\parallel} = P_{i,\parallel}^T \mathcal{F}_i, \quad \mathcal{F}_{i,\perp} = P_{i,\perp}^T \mathcal{F}_i = (I - P_{i,\parallel})^T \mathcal{F}_i.
\end{equation}

To verify orthogonality, for any $\mathcal{V} \in \text{range}(P_{i,\perp})$, we have $\mathcal{V} = P_{i,\perp} \mathcal{V}'$ for some $\mathcal{V}'$, and:
\begin{align}
\mathcal{F}_{i,\parallel}^T \mathcal{V} &= (P_{i,\parallel}^T \mathcal{F}_i)^T (P_{i,\perp} \mathcal{V}') = \mathcal{F}_i^T P_{i,\parallel} P_{i,\perp} \mathcal{V}' = 0,
\end{align}
where the last equality follows immediately from the orthogonality of the twist projectors ($P_{i,\parallel} P_{i,\perp} = 0$). Similarly, it can be shown that $\mathcal{F}_{i,\perp}^T \mathcal{V} = 0$ for all $\mathcal{V} \in \text{range}(P_{i,\parallel})$.

The parallel component $\mathcal{F}_{i,\parallel}$ represents productive wrench that performs work along the object's internal degree of freedom, contributing to desired joint motion. The orthogonal component $\mathcal{F}_{i,\perp}$ represents \textbf{internal wrench} that:
\begin{itemize}
    \item Does not contribute to desired object motion along the screw axis (zero virtual power along $\text{range}(P_{i,\parallel})$)
    \item Arises from coordination errors between the two arms
    \item Represents constraint forces (bearing loads, friction, etc.) unrelated to joint actuation
    \item Increases unnecessary contact stress and grasp instability
    \item Wastes energy and risks hardware damage
\end{itemize}

By penalizing $\|\mathcal{F}_{i,\perp}\|_2^2$, the policy learns to minimize non-productive forces while maintaining necessary productive forces for manipulation. This wrench decomposition is fully consistent with the twist decomposition framework, utilizing the duality of the learned kinematic structure.

\textbf{Regularization ($r_{\text{reg}}$).} Encourages smooth motion:

\begin{equation}
\label{eq:reward_regularization}
r_{\text{reg}} = - w_{reg} \sum_{i \in \lbrace l,r \rbrace} \|\dot{\mathcal{V}}_i\|^2.
\end{equation}

This reduces energy consumption (torque magnitude), joint jerkiness (joint acceleration), and Cartesian jerkiness (twist acceleration), promoting natural and efficient movements.

\textbf{Termination Conditions.} To ensure grasp stability, episodes terminate early (task failure) if grasp drift exceeds safety thresholds:

\begin{equation}
\text{Terminate if: } \exists i \in \{l, r\} \text{ such that } \left\|\left[\log\left((T_{\text{grip},i}^{\text{init}})^{-1} T_{\text{grip},i}\right)\right]^\vee\right\|_2 > d_{\max},
\end{equation}

where $T_{\text{grip},i}^{\text{init}}$ is the initial grasp pose, $T_{\text{grip},i}$ is the current grasp pose, and $d_{\max}$ is the maximum allowable drift threshold. This geodesic distance on SE(3) captures both translational and rotational drift from the initial grasp configuration. When this threshold is exceeded, the episode terminates immediately with a failure signal, encouraging the policy to maintain stable grasps throughout manipulation without explicit reward shaping.

\subsubsection{Policy Network Architecture}

The Low-Level Policy $\pi_\theta: \mathcal{S} \to \mathcal{A}$ is implemented as a neural network with \textbf{object-conditioned multi-stream architecture}. The network employs Feature-wise Linear Modulation (FiLM) to inject object geometric structure into all feature processing stages, enabling constraint-aware representation learning.

For detailed architecture specifications, see Appendix~\ref{app:network_architecture}.

\subsubsection{Training Procedure}

We employ \textbf{Proximal Policy Optimization (PPO)} with standard hyperparameters for policy gradient updates.

\textbf{PPO Objective:}

\begin{equation}
\label{eq:ppo}
L^{CLIP}(\theta) = \mathbb{E}_t \left[\min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t\right)\right],
\end{equation}

where $r_t(\theta) = \pi_\theta(a_t|o_t) / \pi_{\theta_{\text{old}}}(a_t|o_t)$ and advantages are computed via Generalized Advantage Estimation (GAE):

\begin{equation}
\label{eq:gae}
\hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}, \quad \delta_t = r_t + \gamma V_\phi(o_{t+1}) - V_\phi(o_t).
\end{equation}

where $V_\phi: \mathcal{O} \to \mathbb{R}$ is the value function approximated by a separate critic network with architecture identical to the policy encoder (shared encoders, separate value head).

\textbf{Algorithm Summary:} The complete training procedure is summarized in Algorithm~\ref{alg:SWIVL_training}.

\begin{algorithm}[t]
\caption{SWIVL Training}
\label{alg:SWIVL_training}
\begin{algorithmic}[1]
\REQUIRE Pre-trained High-Level Policy $\pi_{HL}$, object set $\mathcal{O}_{\text{obj}}$
\STATE Initialize: Low-Level Policy parameters $\theta$, value function parameters $\phi$
\FOR{episode = 1 to $N_{\text{episodes}}$}
    \STATE Sample object with task $o \sim \mathcal{O}_{\text{obj}}$
    \STATE Initialize robot, environment, and action chunk buffer
        \FOR{$t = 1$ to $H$}
            \STATE \textbf{High-Level Policy}
            \IF{$t \mod f_{HL}^{-1} == 0$}
                \STATE Generate action chunk: $\{T_{sd_i}[\tau]\}_{\tau=0}^{H_{\text{chunk}}} \gets \pi_{HL}$
            \ENDIF
            \STATE \textbf{Reference Motion Field Generator}
            \STATE Interpolate action chunk $\to$ dense trajectory $T_{sd_i}(t)$
            \STATE Compute desired body twists $\mathcal{V}_i^{\text{des}}(t)$ via Eq.~\ref{eq:body_twist}
            \STATE Apply stable vector field $\to$ reference twists $\mathcal{V}_i^{\text{ref}}$ via Eq.~\ref{eq:vector_field}
            \STATE Decompose $\mathcal{V}_i^{\text{ref}} \to$ internal motion $\mathcal{V}_{i,\parallel}^{\text{ref}}$ and bulk motion $\mathcal{V}_{i,\perp}^{\text{ref}}$
            \STATE \textbf{Low-Level Policy}
            \STATE Observe $o_t = (\mathcal{V}_i^{\text{ref}}, \mathcal{B}_i, \mathcal{F}_i, T_{sb_i}, \mathcal{V}_i)$
            \STATE Sample action $a_t = (d_{i,\parallel}, d_{i,\perp}, k_{p_i}, \alpha) \sim \pi_\theta(\cdot|o_t)$
            \STATE \textbf{Controller}
            \STATE Compute commanded wrench $\mathcal{F}_{\mathrm{cmd}, i}$ via Eq.~\ref{eq:impedance_control_main}
            \STATE Execute joint torque $\tau_{\mathrm{cmd}, i} = J_i(\theta_i)^T \mathcal{F}_{\mathrm{cmd}, i}$ via Eq.~\ref{eq:joint_torque}
            \STATE \textbf{Collect experience}
            \STATE Observe $o_{t+1}$, compute reward $r_t$ via Eq.~\ref{eq:reward}
            \STATE Store transition $(o_t, a_t, r_t, o_{t+1})$
        \ENDFOR
    \STATE \textbf{PPO Update}
    \STATE Compute advantages via GAE (Eq.~\ref{eq:gae})
    \FOR{epoch = 1 to 10}
        \STATE Sample mini-batches from buffer
        \STATE Update $\theta$ via Eq.~\ref{eq:ppo}
        \STATE Update $\phi$ via MSE loss on value targets
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}
