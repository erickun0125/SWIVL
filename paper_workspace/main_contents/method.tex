% Method - Revised Version (Comprehensive)
\section{Method}
\label{sec:method}

We present \textbf{SWIVL} (Screw-Wrench informed Impedance Variable Learning), a hierarchical control framework that bridges high-level cognitive planning with physically grounded bimanual execution. SWIVL consists of three key components: (1) a Stable Imitation Vector Field based \textbf{Reference Twist Field Generator} that transforms discrete high-level waypoints into dense, continuous vector fields defined over the entire task space, (2) a Screw Axes Decomposition based \textbf{Twist-driven Impedance Controller} that enables practical impedance control with pose error and twist error, and (3) a Reinforcement Learning based \textbf{Wrench-feedback and Object-conditioned Impedance Variable Learning Policy} that modulates compliance in a physically feasible manner.

%==============================================================================
\subsection{Problem Setup and Key Challenges}
\label{sec:problem_setup}
%==============================================================================

We address \textbf{bimanual manipulation of articulated objects} where two robot arms cooperatively manipulate a shared object with $k$ internal degrees of freedom. Let $T_{sb_l}, T_{sb_r} \in \mathrm{SE}(3)$ denote the end-effector poses and ${}^s\mathcal{V}_{b_l}, {}^s\mathcal{V}_{b_r}\in \mathbb{R}^6\cong \mathfrak{se}(3)$ denote the end-effector twists in the space frame.
For articulated objects, $\mathbf{q}_{\text{obj}}, \dot{\mathbf{q}}_{\text{obj}} \in \mathbb{R}^k$ are the joint positions and joint velocities, and the kinematic structure is characterized by the 
spatial Jacobian $\mathbf{J}_s(\mathbf{q}_{\text{obj}}) \in \mathbb{R}^{6 \times k}$, whose columns correspond to the \textit{instantaneous} spatial screw axes determined by the current robot and joint configuration.

\textbf{Holonomic Constraint.} Since both end-effectors rigidly grasp the object, their relative motion is constrained to match the motion generated by the object's internal joints. This relationship is expressed in the spatial twist domain as:
\begin{equation}
\label{eq:holonomic_constraint}
{}^s\mathcal{V}_{b_l} - {}^s\mathcal{V}_{b_r} = \mathbf{J}_s(\mathbf{q}_{\text{obj}}) \dot{\mathbf{q}}_{\text{obj}},
\end{equation}
where
\begin{itemize}
    \item ${}^s\mathcal{V}_{b_l}, {}^s\mathcal{V}_{b_r} \in \mathbb{R}^6$ denote the spatial twists of the left and right end-effectors, respectively.
    \item $\dot{\mathbf{q}}_{\text{obj}} \in \mathbb{R}^k$ represents the joint velocity vector.
\end{itemize}

\textbf{Goal.} Given high-level waypoints $\{T_{sd_i}[\tau]\}_{\tau=0}^{H}$ from cognitive planners (VLAs, behavior cloning policies, or teleoperation) that lack physical awareness, develop a low-level control stack that: (i) generates dense, stable reference motions, (ii) satisfies kinematic constraints compliantly, and (iii) minimizes fighting forces while tracking references.

\textbf{Key Challenges.} This problem presents four interrelated challenges that directly motivate SWIVL's architectural components:

\begin{enumerate}[label=\textbf{C\arabic*}., leftmargin=2.5em]
    \item \textbf{From Discrete Poses to Dense Twist References.} Modern learning-based policies output \textbf{action chunks}---sequences of waypoints enabling temporal consistency and multi-step reasoning. However, these are inherently \textbf{sparse} (discrete snapshots vs. continuous control) and \textbf{open-loop} (no corrective feedback when deviations occur due to inter-arm forces). More fundamentally, there exists a \textbf{pose space vs. twist space mismatch}: kinematic constraints and impedance motions are linear in $\mathfrak{se}(3)$ but nonlinear in $\mathrm{SE}(3)$.
    
    \item \textbf{Compliant Constraint Satisfaction.} A natural approach is to structurally enforce Eq.~\eqref{eq:holonomic_constraint} in the action space. However, this \textbf{hard constraint approach is brittle}: any small error in $\mathbf{J}_s$ due to perception noise, calibration errors, or object model mismatch directly translates to physically infeasible commands. When executed via high-stiffness control, these generate large fighting forces risking grasp slippage or hardware failure.
    
    \item \textbf{SE(3) Impedance Complexity.} Impedance control on SE(3) involves a nonlinear Jacobian $J_{\mathcal{E}}$ that couples rotation and translation in configuration-dependent ways (Appendix~\ref{app:impedance}). Commonly used approximations---small orientation errors and diagonal stiffness matrices---fail in our scenario due to large trajectory deviations from constraint violations and non-diagonal stiffness requirements for motion decomposition.
    
    \item \textbf{Bulk Motion Ambiguity.} Directly tracking raw end-effector references is ill-suited for object-aware manipulation. Instead, the control objective effectively requires decoupling into \textit{internal motion} and \textit{bulk motion} . However, a fundamental conflict arises when high-level planners generate kinematically inconsistent commands, the bulk motion (orthogonal part to internal joint motions) components of $\mathcal{V}_l^{\text{ref}}$ and $\mathcal{V}_r^{\text{ref}}$ do not match. This ambiguity manifests as \textbf{inter-arm force coupling}: wrench components orthogonal to screw axes directly reflect bulk motion disagreement.
\end{enumerate}

\textbf{Framework Scope.} We develop the theoretical framework in SE(3) with multi-DoF articulated objects for generality, while experimental validation is conducted in SE(2) with 1-DoF objects to isolate core challenges.

\textbf{Notation.} We use $\{s\}$ for spatial frame, $\{b_i\}$ for body frame of end-effector $i \in \{l, r\}$, $\{d_i\}$ for desired frame, $T_{ab}$ for transformation from $\{b\}$ to $\{a\}$, and ${}^a\mathcal{V}_b$ for twist of frame $\{b\}$ expressed in $\{a\}$.

%==============================================================================
\subsection{Architecture Overview}
\label{sec:architecture_overview}
%==============================================================================

SWIVL adopts a four-layer hierarchical architecture (Figure~\ref{fig:architecture_overview}) that decouples high-level reasoning from low-level physical interaction:

\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Layer 1 (High-Level Policy)}: VLA, behavior cloning, or teleoperation providing sparse waypoints $\{T_{sd_i}[\tau]\}_{\tau=0}^{H}$ at low frequency ($\sim$10Hz).
    
    \item \textbf{Layer 2 (Reference Twist Field Generator)}: Transforms sparse waypoints into dense, closed-loop reference twists $\mathcal{V}_i^{\text{ref}}$ with stability guarantees (\textbf{C1}).
    
    \item \textbf{Layer 3 (Impedance Variable Modulation Policy)}: RL policy $\pi_\theta$ that modulates impedance variables based on object geometry and wrench feedback (\textbf{C3, C4}).
    
    \item \textbf{Layer 4 (Screw Axes-Decomposed Impedance Controller)}: Executes compliant control with independent regulation of bulk and internal motions (\textbf{C2, C3}).
\end{itemize}

The core innovation lies in the tight integration of Layers 2--4, which together enable physically grounded, force-compliant bimanual manipulation underneath arbitrary high-level planners.

% ============================================================================
% FIGURE PLACEHOLDER: Architecture Overview
% ============================================================================
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figures/architecture_overview.pdf}
%     \caption{\textbf{SWIVL Architecture.} Four-layer hierarchy bridging cognitive planning with physical execution. High-level waypoints are transformed into stable reference twists (Layer 2), which are tracked by a screw-decomposed impedance controller (Layer 4) with learned compliance modulation (Layer 3). Wrench feedback enables adaptive force regulation.}
%     \label{fig:architecture_overview}
% \end{figure}
% ============================================================================

%==============================================================================
\subsection{Stable Imitation Vector Field (Layer 2)}
\label{sec:reference_twist_field}
%==============================================================================

This layer addresses \textbf{C1} by bridging the gap between discrete high-level waypoints and continuous low-level control. High-level policies provide sparse waypoints at low frequency ($\sim$10Hz), while the low-level controller requires smooth, dense reference trajectories at high frequency ($\sim$100Hz) with pose-error based corrective fields.

A key insight is the \textbf{pose space vs. twist space mismatch}: high-level planners naturally output poses $T \in \mathrm{SE}(3)$, but kinematic constraints and impedance control are fundamentally more tractable in the twist space $\mathfrak{se}(3)$. Although Equation~\eqref{eq:holonomic_constraint} presents a linear relationship in the velocity domain, the fundamental geometric constraint resides in the configuration space $SE(3)$. For a general articulated object defined by a kinematic chain of $k$ joints, the relative pose between the two end-effectors is governed by the Product of Exponentials (POE) formula:
\begin{equation}
\label{eq:se3_constraint}
T_{s b_l}^{-1} T_{s b_r} = \left( \prod_{i=1}^{k} e^{[\mathcal{S}_i] q_i} \right) M_{\text{obj}} \,,
\end{equation}
where:
\begin{itemize}
    \item $T_{s b_l}, T_{s b_r} \in SE(3)$ represent the current spatial poses of the left and right end-effectors.
    \item $\mathcal{S}_i \in \mathbb{R}^6$ denotes the spatial screw axis of the $i$-th joint at the zero configuration.
    \item $q_i$ is the position of the $i$-th joint.
    \item $M_{\text{obj}} \in SE(3)$ is the constant relative transformation between the end-effectors at the zero configuration ($\mathbf{q}_{\text{obj}} = \mathbf{0}$).
\end{itemize}
Nonlinear constraints in SE(3) manifold (Eq.
\eqref{eq:se3_constraint})
motivates transforming pose commands into twist-space representations where constraints are linear in vector space.

The Reference Twist Field Generator performs three steps:

\paragraph{Step 1: SE(3) Trajectory Smoothing.} 
To address the sparsity gap, we perform smooth interpolation in SE(3) to obtain dense desired trajectories at the control frequency $\Delta t_{LL} \ll \Delta t_{HL}$:
\begin{equation}
\{T_{sd_l}(t), T_{sd_r}(t)\}_{t=0}^{H_{LL}},
\end{equation}
where $H_{LL}$ is the smoothed trajectory horizon. We use SLERP for rotations and cubic splines for translations. Both schemes are differentiable in time, inducing continuous position and rotation trajectories $p_i^{\text{des}}(t)$ and $R_i^{\text{des}}(t)$.

\paragraph{Step 2: Body Twist Computation.} 
For an interpolated trajectory $T_{sd_i}(t) = \begin{bsmallmatrix} R_{sd_i}(t) & p_{sd_i}(t) \\ 0 & 1 \end{bsmallmatrix}$, we compute the desired body twist by differentiation:
\begin{equation}
\label{eq:body_twist}
\mathcal{V}_i^{\text{des}}(t) =
\begin{bmatrix}
\omega_i^{\text{des}}(t) \\
v_i^{\text{des}}(t)
\end{bmatrix},
\quad [\omega_i^{\text{des}}(t)]_\times = R_{sd_i}(t)^{\top} \dot{R}_{sd_i}(t),
\quad v_i^{\text{des}}(t) = R_{sd_i}(t)^{\top} \dot{p}_{sd_i}(t),
\end{equation}
where $[\omega]_\times$ denotes the skew-symmetric matrix of $\omega$. Both $\omega_i^{\text{des}}(t)$ and $v_i^{\text{des}}(t)$ are expressed in the desired frame $\{d_i\}$ by construction.

\paragraph{Step 3: Stable Imitation Vector Field.} 
As execution progresses, actual end-effector poses may deviate significantly from the desired trajectory due to tracking errors, disturbances, and model mismatch. To address the \textbf{open-loop nature} of waypoints and provide robustness, we construct a vector field that balances \textbf{imitation} of demonstrated motions and \textbf{stability} for error correction:
\begin{equation}
\label{eq:vector_field}
\mathcal{V}_i^{\text{ref}}(t, T_{sb_i}) = \mathrm{Ad}_{T_{b_id_i}}\mathcal{V}_i^{\text{des}}(t) + k_{p_i} \mathcal{E}_i,
\end{equation}
where $\mathrm{Ad}_{T_{b_id_i}}$ is the adjoint transformation mapping the desired twist from frame $\{d_i\}$ to the current body frame $\{b_i\}$. The transformation $T_{b_id_i} = (T_{sb_i})^{-1} T_{sd_i}$ represents the relative pose from desired to current frame. The pose error term is:
\begin{equation}
\label{eq:pose_error}
\mathcal{E}_i = \begin{pmatrix} \alpha \, e_{R_i} \\ e_{p_i} \end{pmatrix} \in \mathbb{R}^6, \quad
e_{R_i} = \log(R_{sb_i}^\top R_{sd_i})^\vee, \quad
e_{p_i} = R_{sb_i}^\top(p_{sd_i} - p_{sb_i}),
\end{equation}
where $e_{R_i} \in \mathbb{R}^3$ is the rotation error and $e_{p_i} \in \mathbb{R}^3$ is the translation error, both expressed in the body frame. Here $\alpha \in \mathbb{R}$ is a characteristic length weighting rotation vs. translation from the  riemannian metric(Appendix~\ref{app:impedance}), and $k_{p_i} \in \mathbb{R}$ is a  proportional gain of corrective field for pose error.

\textbf{Metric Tensor.} The characteristic length $\alpha$ defines a metric tensor $G = \mathrm{diag}(\alpha^2 I_3, I_3)$ inducing an inner product on $\mathfrak{se}(3)$. Physically, this inner product corresponds to the \textit{kinetic energy} of an isotropic rigid body with unit mass and scalar moment of inertia $\alpha^2 I_3$:
\begin{equation}
\label{eq:inner_product}
\langle \mathcal{V}_1, \mathcal{V}_2 \rangle_G = \frac{\alpha^2}{2} \mathrm{tr}([\omega_1]^\top [\omega_2]) + v_1^\top v_2 = \mathcal{V}_1^\top G \mathcal{V}_2,
\end{equation}
Here, $\alpha$ acts as the radius of gyration, representing the ratio between the body's rotational and translational inertia. Furthermore, this metric induces a left-invariant Riemannian metric on $SE(3)$ with a distinct geometric significance: its geodesics correspond to the \textit{free-body motion} of said isotropic body, thereby providing a dynamically consistent notion of distance on the manifold.

\textbf{Dual Role of $k_p \mathcal{E}$.} The stability term $k_{p_i} \mathcal{E}_i$ serves a crucial dual purpose: it provides corrective feedback for trajectory tracking \emph{and} acts as an elastic force term in the impedance framework. As we show in Section~\ref{sec:controller_interpretation}, this formulation \textbf{bypasses the problematic SE(3) Jacobian $J_{\mathcal{E}}$} while maintaining impedance behavior.

%==============================================================================
\subsection{Screw Axes-Decomposed Impedance Control (Layer 4)}
\label{sec:screw_decomposed_control}
%==============================================================================

This layer addresses \textbf{C2} and \textbf{C3} by enabling compliant control with independent regulation of bulk and internal motions. Rather than enforcing hard kinematic constraints (which is brittle under model uncertainty), SWIVL adopts a \textbf{compliant approach} through impedance control, providing soft regulation of constraint violations while minimizing harmful bulk forces.

The key insight is to \emph{decompose} the twist space into object-centric subspaces aligned with the object's screw axes.

\subsubsection{Motion Space Decomposition}
\label{sec:motion_decomposition}

In articulated object manipulation, tracking is fundamentally object-centric. To maximally reflect the task semantics embedded in the high-level cognitive planner's trajectory, it is essential to interpret the motion relative to the object structure. Consequently, rather than independently tracking each arm's reference motion, it is advantageous to decompose motions into:
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Internal motion} $\mathcal{V}_\parallel \in \text{range}(J_i)$: drives joint articulation
    \item \textbf{Bulk motion} $\mathcal{V}_\perp$: drives the object's overall motion through space
\end{itemize}
Tracking these components separately provides clearer learning signals aligned with the object's kinematic structure.

Let $J_i(\mathbf{q}_{\text{obj}}) \in \mathbb{R}^{6 \times k}$ denote the body Jacobian encoding how object joint velocities $\dot{\mathbf{q}}_{\text{obj}}$ manifest as end-effector body twists. The body Jacobian relates to the spatial Jacobian via the adjoint: $J_i = \mathrm{Ad}_{T_{b_is}} \mathbf{J}_s$, where $T_{b_is} = (T_{sb_i})^{-1}$.

Using the metric tensor $G = \mathrm{diag}(\alpha^2 I_3, I_3)$ from Eq.~\eqref{eq:inner_product}, we construct $G$-orthogonal projection operators:
\begin{equation}
\label{eq:projection_operators}
P_{i,\parallel} = J_i (J_i^\top G J_i)^{-1} J_i^\top G, \quad P_{i,\perp} = I - P_{i,\parallel},
\end{equation}
where $P_{i,\parallel}$ projects onto the internal motion subspace (range of $J_i$) and $P_{i,\perp}$ projects onto the bulk motion subspace (orthogonal complement). These projectors satisfy:
\begin{itemize}[leftmargin=1.5em]
    \item $G$-self-adjointness: $P_{i,\parallel}^\top G = G P_{i,\parallel}$ and $P_{i,\perp}^\top G = G P_{i,\perp}$
    \item Orthogonality: $P_{i,\parallel} P_{i,\perp} = 0$
    \item Partition of identity: $P_{i,\parallel} + P_{i,\perp} = I$
\end{itemize}
Importantly, since $\alpha$ is learned by the policy (Section~\ref{sec:impedance_policy}), the metric tensor $G$ and hence the projection operators adapt dynamically to task requirements, enabling context-dependent orthogonal decomposition.

Twists decompose as:
\begin{align}
\mathcal{V}_{i,\parallel}^{\text{ref}} &= P_{i,\parallel} \mathcal{V}_i^{\text{ref}}, \quad \mathcal{V}_{i,\parallel} = P_{i,\parallel} \mathcal{V}_i \quad \text{(internal motion)}, \label{eq:twist_decomp_internal}\\
\mathcal{V}_{i,\perp}^{\text{ref}} &= P_{i,\perp} \mathcal{V}_i^{\text{ref}}, \quad \mathcal{V}_{i,\perp} = P_{i,\perp} \mathcal{V}_i \quad \text{(bulk motion)}. \label{eq:twist_decomp_bulk}
\end{align}

\subsubsection{Controller Formulation}
\label{sec:controller_formulation}

Given impedance variables $(d_{i,\parallel}, d_{i,\perp}, k_{p_i}, \alpha)$ from the policy, we construct a damping matrix respecting the motion decomposition:
\begin{equation}
\label{eq:damping_matrix}
K_{d_i} = G (P_{i,\parallel} d_{i,\parallel} + P_{i,\perp} d_{i,\perp}),
\end{equation}
enabling independent damping for internal motion (via $d_{i,\parallel}$) and bulk motion (via $d_{i,\perp}$). The policy can adaptively regulate compliance based on task requirements and force feedback.

The commanded wrench is:
\begin{equation}
\label{eq:impedance_control_main}
\begin{aligned}
\mathcal{F}_{\mathrm{cmd}, i} &= K_{d_i} (\mathcal{V}_i^{\text{ref}} - \mathcal{V}_i) + \mu_{b,i} + \gamma_{b,i}\\
&= K_{d_i} (\mathrm{Ad}_{T_{b_id_i}} \mathcal{V}_i^{\text{des}} - \mathcal{V}_i + k_{p_i} \mathcal{E}_i) + \mu_{b,i} + \gamma_{b,i},
\end{aligned}
\end{equation}
where $\mathcal{V}_i^{\text{ref}} = \mathrm{Ad}_{T_{b_id_i}} \mathcal{V}_i^{\text{des}} + k_{p_i} \mathcal{E}_i$ is the reference twist from Layer 2, $\mu_{b,i}$ compensates for Coriolis/centrifugal terms, and $\gamma_{b,i}$ provides gravity compensation (omitted in planar SE(2) settings).

The commanded wrench maps to joint torques via the manipulator Jacobian:
\begin{equation}
\label{eq:joint_torque}
\tau_{\mathrm{cmd}, i} = J_i(\theta_i)^\top \mathcal{F}_{\mathrm{cmd}, i},
\end{equation}
where $J_i(\theta_i) \in \mathbb{R}^{6 \times n}$ is the geometric Jacobian mapping joint velocities to end-effector twist.

\subsubsection{Dual Interpretation of the Controller}
\label{sec:controller_interpretation}

The controller admits two complementary interpretations that reveal its power.

\paragraph{Interpretation 1: Twist-Driven SE(3) Impedance.}
Classical SE(3) impedance control designs a virtual dynamical system:
\begin{equation}
M \dot{\xi} + D \xi + J_{\mathcal{E}}^\top K \mathcal{E} = \mathcal{F}_{\mathrm{ext}},
\end{equation}
where $\xi = {}^b\mathcal{V}_d - {}^b\mathcal{V}_b$ is velocity error, $M$ is desired inertia, $D$ is damping, and $J_{\mathcal{E}}^\top K \mathcal{E}$ is the nonlinear stiffness term arising from SE(3) geometry. The corresponding controller takes the form:
\begin{equation}
\mathcal{F}_{\mathrm{cmd}} = \Lambda_b M^{-1}(D\xi + J_{\mathcal{E}}^\top K \mathcal{E}) + \Lambda_b {}^b\dot{\mathcal{V}}_d + \mu_b + \gamma_b + (I - \Lambda_b M^{-1})\mathcal{F}_{\mathrm{ext}},
\end{equation}
where $\Lambda_b$ is the operational space inertia. Under common simplifications $M = \Lambda_b$ and ${}^b\dot{\mathcal{V}}_d = 0$, this reduces to:
\begin{equation}
\mathcal{F}_{\mathrm{cmd}} = D\xi + J_{\mathcal{E}}^\top K \mathcal{E} + \mu_b + \gamma_b.
\end{equation}

Our controller in Eq.~\eqref{eq:impedance_control_main} follows this structure. Defining velocity error $\xi = \mathrm{Ad}_{T_{b_id_i}} \mathcal{V}_i^{\text{des}} - \mathcal{V}_i$:
\begin{equation}
\label{eq:impedance_equivalence}
\begin{aligned}
\mathcal{F}_{\mathrm{cmd}, i} &= K_{d_i} \xi + K_{d_i} k_{p_i} \mathcal{E}_i + \mu_{b,i} + \gamma_{b,i}\\
&\approx D\xi + J_{\mathcal{E}}^\top K \mathcal{E} + \mu_b + \gamma_b,
\end{aligned}
\end{equation}
with correspondence $D \leftrightarrow K_{d_i}$ (learned damping) and $J_{\mathcal{E}}^\top K \mathcal{E} \leftrightarrow K_{d_i} k_{p_i} \mathcal{E}_i$ (stiffness). Critically, our approach \textbf{sidesteps the explicit nonlinear Jacobian $J_{\mathcal{E}}$} by incorporating pose error directly into the reference twist, avoiding geometric complications while maintaining impedance behavior.

\paragraph{Interpretation 2: Task-Semantic Motion Decomposition.}
Decomposing twists via Eqs.~\eqref{eq:twist_decomp_internal}--\eqref{eq:twist_decomp_bulk}, the control law expands to:
\begin{equation}
\label{eq:decomposed_control}
\begin{aligned}
\mathcal{F}_{\mathrm{cmd}, i} &= G(P_{i,\parallel} d_{i,\parallel} + P_{i,\perp} d_{i,\perp})(\mathcal{V}_i^{\text{ref}} - \mathcal{V}_i) + \mu_{b,i} + \gamma_{b,i}\\
&= \underbrace{d_{i,\parallel} G (\mathcal{V}_{i,\parallel}^{\text{ref}} - \mathcal{V}_{i,\parallel})}_{\text{internal motion control}} + \underbrace{d_{i,\perp} G (\mathcal{V}_{i,\perp}^{\text{ref}} - \mathcal{V}_{i,\perp})}_{\text{bulk motion control}} + \mu_{b,i} + \gamma_{b,i}.
\end{aligned}
\end{equation}

This decomposition provides two critical properties:

\begin{enumerate}
    \item \textbf{Independent compliance modulation}: The policy can independently adjust $d_{i,\parallel}$ and $d_{i,\perp}$ for task-specific behavior---high stiffness for bulk motion during transport, high compliance for bulk motion during articulation, or vice versa. It gives implicit constraint satisfaction ensuring compliant motions respect the object's kinematic constraints.
    
    \item \textbf{Decoupled power generation}: The feedback wrenches for internal and bulk motions are \textbf{reciprocally orthogonal} to opposing motion subspaces:
    \begin{align}
        \mathcal{F}_{\mathrm{cmd,fb}, i,\parallel} &= d_{i,\parallel} G (\mathcal{V}_{i,\parallel}^{\text{ref}} - \mathcal{V}_{i,\parallel}), \label{eq:wrench_fb_internal}\\
        \mathcal{F}_{\mathrm{cmd,fb}, i,\perp} &= d_{i,\perp} G (\mathcal{V}_{i,\perp}^{\text{ref}} - \mathcal{V}_{i,\perp}), \label{eq:wrench_fb_bulk}\\
        (\mathcal{F}_{\mathrm{cmd,fb}, i,\parallel})^\top (\mathcal{V}_{i,\perp}^{\text{ref}} - \mathcal{V}_{i,\perp}) &= 0, \label{eq:orthogonality_1}\\
        (\mathcal{F}_{\mathrm{cmd,fb}, i,\perp})^\top (\mathcal{V}_{i,\parallel}^{\text{ref}} - \mathcal{V}_{i,\parallel}) &= 0. \label{eq:orthogonality_2}
    \end{align}
    This orthogonality follows from $P_{i,\parallel}^\top G P_{i,\perp} = 0$, ensuring control actions for each motion type do not interfere.
\end{enumerate}
Together, these interpretations demonstrate that our controller achieves geometrically consistent SE(3) impedance behavior while enabling explicit, learning-based modulation of task-semantic motion components---a capability that would be intractable to design analytically.

%==============================================================================
\subsection{Wrench-Adaptive Impedance Learning (Layer 3)}
\label{sec:impedance_policy}
%==============================================================================

This layer addresses \textbf{C3} and \textbf{C4} by learning adaptive impedance modulation. The RL policy $\pi_\theta: \mathcal{O} \to \mathcal{A}$ modulates impedance variables to enable physically feasible motions while accounting for object constraints and inter-arm force interactions. By explicitly conditioning on object geometry (screw axes) and wrench feedback, the policy learns to independently modulate compliance for bulk versus internal motions and mitigating unnecessary bulk forces.

\subsubsection{Observation and Action Spaces}
\textbf{Observation Space $\mathcal{O}$.} The policy receives:
\begin{enumerate}
    \item \textbf{Reference twists}: $\{\mathcal{V}_l^{\text{ref}}, \mathcal{V}_r^{\text{ref}}\} \in \mathfrak{se}(3) \times \mathfrak{se}(3)$\\
    Reference motions computed by Reference Twist Field Generator (Layer 2) with desired twist and pose error correction.
    
    \item \textbf{Object kinematic structure}: \\
For a $k$-DoF articulated object, 
    \begin{itemize}
        \item \textbf{Inter-link screw axes}: $\{\mathcal{A}_2, \ldots, \mathcal{A}_{k-1}\}$ where $\mathcal{A}_i \in \mathfrak{se}(3)$ is the screw axis of joint $i$ expressed in the $(i-1)$-th link frame, describing the relative motion from link $(i-1)$ to link $i$.
        \item \textbf{Body Screw of $1^{th}, k^{th}$ joint}: $\{\mathcal{B}_l, \mathcal{B}_r\} \in \mathfrak{se}(3) \times \mathfrak{se}(3)$\\
Represents body-frame screw axes of  $1^{th}, k^{th}$ joint at each corresponding end-effector. 
    \end{itemize}
    For \textbf{1-DoF objects}, the kinematic structure reduces to the body-frame screw axes $\{\mathcal{B}_l, \mathcal{B}_r\} \in \mathfrak{se}(3) \times \mathfrak{se}(3)$. This simplified representation is sufficient for our experimental validation in SE(2) with single-joint articulated objects.
    
    \item \textbf{Wrench feedback}: $\{\mathcal{F}_l, \mathcal{F}_r\} \in \mathfrak{se}(3)^* \times \mathfrak{se}(3)^*$\\
    6-dimensional wrench measurements (3D moment + 3D force) from wrist-mounted F/T sensors. 
    
    \item \textbf{Proprioception}: $\{T_{sb_l}, T_{sb_r}, \mathcal{V}_l, \mathcal{V}_r\}  \in \mathrm{SE}(3) \times \mathrm{SE}(3)  \times \mathfrak{se}(3) \times \mathfrak{se}(3)$\\
    Task-space states including end-effector poses and body twists.
\end{enumerate}

\textbf{Action Space $\mathcal{A}$.} We propose an impedance variable action space that structurally enables object-aware compliance modulation:
\begin{equation}
\label{eq:action_space}
a_t = (d_{l,\parallel}, d_{r,\parallel}, d_{l,\perp}, d_{r,\perp}, k_{p_l}, k_{p_r}, \alpha) \in \mathbb{R}^7,
\end{equation}
where all variables are positive scalars:
\begin{itemize}[leftmargin=1.5em]
    \item $d_{i,\parallel}$: Damping coefficient for internal motion.
    \item $d_{i,\perp}$: Damping coefficient for bulk motion.
    \item $k_{p_i}$: Stiffness coefficient for the stability term in the reference twist.
    \item $\alpha$: Learnable characteristic length that adaptively modulates the metric tensor.
\end{itemize}

These variables parameterize the low-level controller (Section~\ref{sec:controller_formulation}), enabling adaptive, context-dependent impedance modulation.  Note that, by learning $\alpha$, the policy discovers task-appropriate metric structures for orthogonal decomposition.

\subsubsection{Wrench Decomposition and Force Regulation}
\label{sec:wrench_decomposition}

To resolve the \textbf{bulk motion ambiguity} (\textbf{C4}), we decompose measured wrenches into internal and bulk components. If high-level planners generated kinematically consistent commands, bulk motion components would agree. However, planners lack constraint awareness, resulting in mismatched bulk motions that manifest as inter-arm forces.

By twist-wrench duality, projecting wrenches with transposed twist projectors preserves orthogonality under the reciprocal product (virtual power). We seek wrench components $\mathcal{F}_{i,\parallel}$ and $\mathcal{F}_{i,\perp}$ satisfying:
\begin{align}
\langle \mathcal{F}_{i,\parallel}, \mathcal{V} \rangle &= \mathcal{F}_{i,\parallel}^\top \mathcal{V} = 0 \quad \forall \mathcal{V} \in \text{range}(P_{i,\perp}), \\
\langle \mathcal{F}_{i,\perp}, \mathcal{V} \rangle &= \mathcal{F}_{i,\perp}^\top \mathcal{V} = 0 \quad \forall \mathcal{V} \in \text{range}(P_{i,\parallel}).
\end{align}

This is naturally satisfied by:
\begin{equation}
\label{eq:wrench_decomposition}
\mathcal{F}_{i,\parallel} = P_{i,\parallel}^\top \mathcal{F}_i \quad \text{(internal)}, \qquad
\mathcal{F}_{i,\perp} = P_{i,\perp}^\top \mathcal{F}_i \quad \text{(bulk)}.
\end{equation}

To verify orthogonality: for any $\mathcal{V} \in \text{range}(P_{i,\perp})$, we have $\mathcal{V} = P_{i,\perp} \mathcal{V}'$, and:
\begin{equation}
\mathcal{F}_{i,\parallel}^\top \mathcal{V} = (P_{i,\parallel}^\top \mathcal{F}_i)^\top (P_{i,\perp} \mathcal{V}') = \mathcal{F}_i^\top P_{i,\parallel} P_{i,\perp} \mathcal{V}' = 0,
\end{equation}
where the last equality follows from $P_{i,\parallel} P_{i,\perp} = 0$.

The parallel component $\mathcal{F}_{i,\parallel}$ represents \textbf{internal wrench} performing work along the object's internal DoF. The orthogonal component $\mathcal{F}_{i,\perp}$ represents \textbf{internal wrench} that:
\begin{itemize}[leftmargin=1.5em]
    \item Does not contribute to object's internal joint motion (zero virtual power along range$(P_{i,\parallel})$)
    \item Arises from coordination errors and high-level commands that do not match between the two arms.
    \item May increases contact stress and grasping instability with hardware damage.
\end{itemize}

Minimizing $\|\mathcal{F}_{i,\perp}\|^2$ implicitly resolves the bulk motion ambiguity: the policy learns to track references in a manner achieving coordinated bulk motion while maintaining compliant internal motion.

\subsubsection{Reward Design}
\label{sec:reward_design}

The reward balances three objectives for stable, force-compliant manipulation:
\begin{equation}
\label{eq:reward}
r_t = r_{\text{track}} + r_{\text{safety}} + r_{\text{reg}}.
\end{equation}

\textbf{Motion Tracking ($r_{\text{track}}$).} Encourages reference following with the learned $G$-metric:
\begin{equation}
\label{eq:reward_tracking}
r_{\text{track}} = -w_{\text{track}} \sum_{i \in \{l,r\}} \|\mathcal{V}_i - \mathcal{V}_i^{\text{ref}}\|_G^2.
\end{equation}
Using the $G$-metric ensures tracking error is measured consistently with the impedance framework, with adaptive weighting via the learned $\alpha$.

\textbf{Safe Inter-arm Force Interaction ($r_{\text{safety}}$).} Penalizes bulk forces:
\begin{equation}
\label{eq:reward_safety}
r_{\text{safety}} = -w_{\text{int}} \sum_{i \in \{l,r\}} \|\mathcal{F}_{i,\perp}\|_2^2.
\end{equation}
By penalizing $\|\mathcal{F}_{i,\perp}\|_2^2$, the policy learns to suppress excessive bulk forces while maintaining necessary internal forces. This wrench decomposition is fully consistent with the twist decomposition framework.

\textbf{Motion Smoothness ($r_{\text{reg}}$).} Regularizes twist acceleration:
\begin{equation}
\label{eq:reward_regularization}
r_{\text{reg}} = -w_{\text{reg}} \sum_{i \in \{l,r\}} \|\dot{\mathcal{V}}_i\|^2,
\end{equation}
reducing jerkiness for motion smoothness.

\textbf{Termination Conditions.} Episodes terminate if grasp drift exceeds a threshold:
\begin{equation}
\label{eq:termination}
\text{Terminate if } \exists i \in \{l,r\}: \left\|\left[\log\left((T_{\text{grip},i}^{\text{init}})^{-1} T_{\text{grip},i}\right)\right]^\vee\right\|_G > d_{\max},
\end{equation}
where $T_{\text{grip},i}^{\text{init}}$ and $T_{\text{grip},i}$ are initial and current grasp poses. This geodesic distance on SE(3) captures both translational and rotational drift with consistent metric weighting, encouraging stable grasps without explicit reward shaping.

\subsubsection{Policy Architecture and Training}

The policy $\pi_\theta: \mathcal{O} \to \mathcal{A}$ uses a multi-stream architecture with \textbf{Feature-wise Linear Modulation (FiLM)}~\cite{perez2018film} to inject object geometry into feature processing. Separate encoders process reference twists, wrenches, and proprioception, with FiLM layers modulating features based on screw axes $\{\mathcal{B}_l, \mathcal{B}_r\}$, enabling dynamic adaptation across joint types.

We train with \textbf{Proximal Policy Optimization (PPO)}~\cite{schulman2017ppo}:
\begin{equation}
\label{eq:ppo}
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[\min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t\right)\right],
\end{equation}
where $r_t(\theta) = \pi_\theta(a_t|o_t) / \pi_{\theta_{\text{old}}}(a_t|o_t)$ and advantages are computed via Generalized Advantage Estimation (GAE)~\cite{schulman2016gae}:
\begin{equation}
\label{eq:gae}
\hat{A}_t = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}, \quad \delta_t = r_t + \gamma V_\phi(o_{t+1}) - V_\phi(o_t),
\end{equation}
where $V_\phi: \mathcal{O} \to \mathbb{R}$ is the value function with architecture identical to the policy encoder. See Appendix~\ref{app:learning_settings} for detailed architecture specifications.

%==============================================================================
% ALGORITHM
%==============================================================================
\begin{algorithm}[t]
\caption{SWIVL Training}
\label{alg:SWIVL_training}
\begin{algorithmic}[1]
\REQUIRE High-Level Policy $\pi_{HL}$, object set $\mathcal{O}_{\text{obj}}$
\STATE Initialize policy parameters $\theta$, value function parameters $\phi$
\FOR{episode = 1 to $N_{\text{episodes}}$}
    \STATE Sample object $o \sim \mathcal{O}_{\text{obj}}$; initialize environment
    \FOR{$t = 1$ to $H$}
        \STATE \textcolor{gray}{\textit{// Layer 1: High-Level Policy}}
        \IF{$t \mod f_{HL}^{-1} = 0$}
            \STATE Generate action chunk $\{T_{sd_i}[\tau]\}_{\tau=0}^{H_{\text{chunk}}} \gets \pi_{HL}$
        \ENDIF
        \STATE \textcolor{gray}{\textit{// Layer 2: Reference Twist Field Generator}}
        \STATE Interpolate action chunk $\to$ dense trajectory $T_{sd_i}(t)$
        \STATE Compute desired body twists $\mathcal{V}_i^{\text{des}}(t)$ via Eq.~\eqref{eq:body_twist}
        \STATE Apply stable vector field $\to$ reference twists $\mathcal{V}_i^{\text{ref}}$ via Eq.~\eqref{eq:vector_field}
        \STATE Decompose $\mathcal{V}_i^{\text{ref}} \to$ internal $\mathcal{V}_{i,\parallel}^{\text{ref}}$ and bulk $\mathcal{V}_{i,\perp}^{\text{ref}}$
        \STATE \textcolor{gray}{\textit{// Layer 3: Impedance Modulation Policy}}
        \STATE Observe $o_t = (\mathcal{V}_i^{\text{ref}}, \mathcal{B}_i, \mathcal{F}_i, T_{sb_i}, \mathcal{V}_i)$
        \STATE Sample $(d_{i,\parallel}, d_{i,\perp}, k_{p_i}, \alpha) \sim \pi_\theta(\cdot|o_t)$
        \STATE \textcolor{gray}{\textit{// Layer 4: Screw-Decomposed Controller}}
        \STATE Compute commanded wrench $\mathcal{F}_{\mathrm{cmd}, i}$ via Eq.~\eqref{eq:impedance_control_main}
        \STATE Execute joint torque $\tau_{\mathrm{cmd}, i} = J_i(\theta_i)^\top \mathcal{F}_{\mathrm{cmd}, i}$
        \STATE \textcolor{gray}{\textit{// Collect Experience}}
        \STATE Observe $o_{t+1}$, compute reward $r_t$ via Eq.~\eqref{eq:reward}
        \STATE Store transition $(o_t, a_t, r_t, o_{t+1})$
    \ENDFOR
    \STATE \textcolor{gray}{\textit{// PPO Update}}
    \STATE Compute advantages via GAE (Eq.~\eqref{eq:gae})
    \FOR{epoch = 1 to $K$}
        \STATE Sample mini-batches; update $\theta$ via Eq.~\eqref{eq:ppo}; update $\phi$ via MSE loss
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}