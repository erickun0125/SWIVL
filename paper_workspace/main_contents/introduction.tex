% Introduction
\section{Introduction}
\label{sec:introduction}

Learning-based robotic manipulation has achieved strong performance in single-arm settings, particularly for structured pick-and-place tasks learned via demonstration. However, extending such approaches to \textbf{dual-arm manipulation of an articulated object} remains challenging. Coordinated bimanual interaction induces rich \textbf{inter-arm force coupling} and must satisfy complex \textbf{object-centric kinematic constraints}. Existing Learning-from-Demonstration (LfD) frameworks---typically grounded in high-stiffness position control with no explicit representation of object kinematics---often generate unstable motions and large internal forces when multiple arms physically interact with a shared object.

\subsection*{Cognitive vs. Physical Intelligence}

To reason about this challenge, we distinguish between two complementary aspects of robot decision-making: \textbf{Cognitive Intelligence} and \textbf{Physical Intelligence}.

\textbf{Cognitive Intelligence} addresses high-level task understanding through semantic reasoning and planning. Modern VLA-based robot foundation models excel at interpreting goals and decomposing instructions via visual-language understanding based on VLM backbones. However, language operates as an abstracted symbolic representation that lacks the resolution needed for precise physical interaction---making it difficult to specify fine-grained motor commands for contact-rich manipulation such as force modulation or coordinated compliance control.

\textbf{Physical Intelligence}, in contrast, ensures safe and stable execution through explicit modeling of dynamics and kinematic principles. This includes regulating contact forces, satisfying geometric constraints, and generating dynamically consistent motions. This dimension remains underexplored in learning-based manipulation.


Most existing work advances Cognitive Intelligence through robot foundation models trained on cross-embodiment datasets. However, this approach introduces three limitations for contact-rich manipulation. First, cross-embodiment training cannot standardize low-level physical signals---wrench feedback, reference frames, and kinematic constraints vary across platforms. Second, imitation learning reproduces demonstrated trajectories without exploring contact dynamics, lacking the adaptive force modulation that reinforcement learning provides. Third, policies output poses executed via high-stiffness control, that can generate excessive forces and constraint violations. These limitations compound in bimanual articulated object manipulation, where force coupling and closed-chain constraints demand physical reasoning beyond current cognitive approaches.
This mismatch motivates methods that \textbf{bridge high-level cognitive policies to low-level physically grounded control}.

\subsection*{Problem Focus and Scope}

We focus on developing a low-level control stack for bimanual manipulation of an articulated object. Our formulation assumes access to (1) 6-axis wrench measurements at each end-effector via wrist-mounted force/torque sensors, and (2) the screw axes of an articulated object. While not all manipulation scenarios provide such information, these assumptions are satisfied in  structured domains---including repetitive assembly, industrial workflows that represent high-value deployment scenarios for dual-arm systems. Recent advances in geometric perception modules—screw-axis estimation (e.g., Screw-Splatting) further expand the applicability of our framework.

In this setting, we use high-level planners (VLA, Imitation Learning Policies, Teleoperation Interfaces) provide motion intentions as desired end-effector poses, without explicitly reasoning about object's physical information or inter-arm force interactions. Our hierarchical control stack transforms these commands into physically feasible bimanual coordination that satisfies kinematic constraints, suppresses internal forces, and ensures compliant interaction—without requiring the high-level planner to reason about any of these physical details.

\subsection{Our Approach : SWIVL}

To address these problems, we introduce \textbf{SWIVL} (\textbf{S}crew-\textbf{W}rench informed \textbf{I}mpedance \textbf{V}ariable \textbf{L}earning). SWIVL enables safe bimanual manipulation with object-aware decomposed impedance control via screw axes and adaptive impedance variable modulation via wrench feedback. This work makes the following contributions:

\begin{enumerate}
    \item \textbf{Twist-Driven SE(3) Impedance Control via Stable Imitation Vector Fields}:  Bypass the nonlinear pose-error Jacobian inherent in SE(3) impedance control by incorporating pose errors directly into reference twists, enabling geometrically consistent compliance even under large trajectory deviations.

    \item \textbf{Screw Axes-Decomposed Twist and Wrench Spaces}: Orthogonal projection operators structurally partition control into internal (joint articulation) and bulk (object transport) components, and dually decompose wrenches into internal forces and bulk forces, enabling independent compliant behavior for each object kinematics aware subspace.

    \item \textbf{Wrench-adaptive Impedance Variable Learning}: A reinforcement learning policy modulates impedance parameters conditioned on object screw axes and real-time wrench feedback streamed from wrist-mounted force-torque sensors, learning to suppress harmful fighting forces while maintaining compliant trajectory tracking across diverse joint types.
\end{enumerate}

We validate SWIVL on SE(2) benchamarks with articulated objects spanning two joint types: revolute and prismatic. Our results demonstrate that SWIVL achieves higher success rates and lower internal forces compared to imitation learning baselines, highlighting the importance of explicit physical modeling for bimanual manipulation of articulated objects.