% Introduction
\section{Introduction}
\label{sec:introduction}

Learning-based robotic manipulation has achieved strong performance in single-arm settings, particularly for structured pick-and-place tasks learned via demonstration. However, extending such approaches to \textbf{dual-arm manipulation of an articulated object} remains challenging. Coordinated bimanual interaction induces rich \textbf{inter-arm force coupling} and must satisfy complex \textbf{object-centric kinematic constraints}. Existing Learning-from-Demonstration (LfD) frameworks---typically grounded in high-stiffness position control with no explicit representation of object kinematics---often generate unstable motions and large internal forces when multiple arms physically interact with a shared object.

\subsection*{Cognitive vs. Physical Intelligence}

To reason about this challenge, we distinguish between two complementary aspects of robot decision-making: \textbf{Cognitive Intelligence} and \textbf{Physical Intelligence}.

\textbf{Cognitive Intelligence} addresses high-level task understanding through semantic reasoning and planning. Modern VLA-based robot foundation models excel at interpreting goals and decomposing instructions via visual-language understanding based on VLM backbones. However, language operates as an abstracted symbolic representation that lacks the resolution needed for precise physical interaction---making it difficult to specify fine-grained motor commands for contact-rich manipulation such as force modulation or coordinated compliance control.

\textbf{Physical Intelligence}, in contrast, ensures safe and stable execution through explicit modeling of dynamics and kinematic principles. This includes regulating contact forces, satisfying geometric constraints, and generating smooth, dynamically consistent motions---critical requirements for force-coupled bimanual manipulation.

Most existing work advances on Cognitive Intelligence through robot foundation models that leverage broad cross-embodiment datasets for rich semantic understanding. However, cross-embodiment training makes it difficult to standardize low-level physical signals---such as end-effector wrench feedback, reference frame definitions, and kinematic constraints---due to varying sensor types and frame conventions across robots. Consequently, foundation models lack explicit access to such information, limiting their ability to reason about contact forces, enforce geometric constraints, or maintain stable cooperative interaction during manipulation.

Moreover, imitation-driven paradigms are well-suited for learning human-like task schemas, but low-level control in physically constrained environments requires instinctive, dynamically consistent behavior, which we find is better realized through reinforcement learning rather than imitation learning.

This mismatch motivates methods that \textbf{bridge high-level cognitive policies to low-level physically grounded control}.

\subsection*{Problem Focus and Scope}

To this end, we focus on developing a low-level control stack for bimanual manipulation of an articulated object. Our formulation assumes access to (1) 6-axis wrench measurements at each end-effector via wrist-mounted force/torque sensors, and (2) screw axes of an articulated object. While not all manipulation scenarios provide such information, structured domains---including repetitive assembly, industrial workflows, and environments with perception modules (e.g., screw-splatting for axis extraction from point clouds)---reasonably support these assumptions.

In this setting, high-level planners (behavior-cloned policies, teleoperation, or VLA) provide motion intentions as desired end-effector poses, without explicitly reasoning about object's physical information or inter-arm force interactions. Conventional high-stiffness position controllers that directly track these desired poses generate excessive contact forces and constraint violations when manipulating articulated objects, as they lack mechanisms to accommodate unforeseen contact dynamics or kinematic restrictions. This creates a fundamental challenge: translating high-level guidance into physically feasible bimanual coordination that simultaneously maintains kinematic consistency and suppresses harmful internal forces while ensuring compliant interaction in task space. 

\subsection{Our Approach : SWIVL}

To address these problems, we introduce \textbf{SWIVL} (\textbf{S}crew-\textbf{W}rench informed \textbf{I}mpedance \textbf{V}ariable \textbf{L}earning). SWIVL enables safe bimanual manipulation with object-aware decomposed impedance control via screw axes and adaptive impedance variable modulation via wrench feedback. This work makes the following contributions:

\begin{enumerate}
    \item \textbf{Stable Imitation Vector Field}:  By generating a reference twist with the pose error, we enable twist-driven impedance control, thereby bypassing the nonlinearity of pose error based elastic wrenches in SE(3) impedance control.

    \item \textbf{Screw-Decomposed Impedance Control}: By decomposing impedance control with screw axes based orthogonal projection, we structurally enforces independent compliance modulation.

    \item \textbf{Impedance Variable Learning with RL Framework}: A reinforcement learning approach for adaptive impedance modulation conditioned on object geometry and wrench feedback, generalizing across joint types and task objectives.

    \item \textbf{Systematic SE(2) Benchmark}: Demonstrating the necessity of SWIVL for robust bimanual manipulation of an articulated object.
\end{enumerate}