% Related Work
\section{Related Work}
\label{sec:related_work}

Our work bridges research in learning-based robotic manipulation, dual-arm coordination, physically grounded control, and stable motion representation. We organize related work around four themes: \textbf{learning paradigms for manipulation}, \textbf{stable motion representation and geometric control}, \textbf{dual-arm coordination and force control}, and \textbf{hierarchical integration of high-level planning with low-level execution}.

\subsection{Learning-Based Robotic Manipulation}

\textbf{Imitation Learning and Behavior Cloning.} Learning from Demonstration (LfD) has emerged as a dominant paradigm for acquiring manipulation skills, with Behavior Cloning (BC) directly mapping observations to actions through supervised learning. Recent LfD policies use generative models like Diffusion Policy~\cite{chi2023diffusion} and ACT~\cite{zhao2023act} to output action chunks for temporal consistency. However, these approaches focus on mimicking demonstrations in high-stiffness position control without reasoning about contact forces or kinematic constraints, and lack closed-loop feedback to handle trajectory deviations.

\textbf{Vision-Language-Action Models and Foundation Models.} Cross-embodiment robot foundation models like RT-1~\cite{brohan2022rt1}, RT-2~\cite{brohan2023rt2}, $\pi_0$~\cite{black2023pi0}, and Octo~\cite{octo2024} leverage large-scale datasets like Open-X~\cite{openx2023} for broad semantic understanding in cognitive capabilities. While these models excel at high-level goal specification and demonstrate impressive generalization to novel objects and instructions, they are trained without explicit access to force/moment feedback, reference frames, or kinematic constraints---information that is inherently embodiment-specific and difficult to standardize across heterogeneous robot platforms. This limitation motivates our approach of decoupling high-level cognitive planning from low-level physically grounded execution.

\textbf{RL for Manipulation.}Reinforcement learning has shown promise for acquiring contact-rich manipulation skills that are difficult to specify through demonstrations alone. Deep RL has been applied to contact-rich tasks like in-hand manipulation~\cite{andrychowicz2020learning} and tool use~\cite{handa2023dextreme}, optimizing physical objectives through environmental interaction.  However, most RL formulations focus on single-task settings and require dense, task-specific reward signals. We leverage RL to learn task-agnostic low-level control that generalizes across manipulation tasks.

\subsection{Dual-Arm Manipulation and Force Control}

\textbf{Classical Dual-Arm Coordination.} Hybrid position/force control~\cite{raibert1981hybrid} decomposes control into position and force subspaces, while object-centric approaches~\cite{uchiyama1988symmetric} unify dual-arm systems as virtual closed-chain mechanisms. Screw theory~\cite{murray1994mathematical} provides geometric foundations for kinematic constraints. While these classical methods provide theoretical foundations for dual-arm coordination, they typically rely on precise analytical models and struggle with complex contact dynamics in articulated object manipulation.

\textbf{Impedance and Compliant Control.} Impedance control~\cite{hogan1985impedance} regulates position-force relationships through virtual mass-spring-damper dynamics. Object impedance control~\cite{schneider2007passivity} extends this to dual-arm settings. However, these methods assume rigid objects and require manual tuning, limiting applicability to articulated manipulation with varying constraints.

\textbf{Learning Force-Aware Manipulation.} Recent works learn force-sensitive skills through force-conditioned policies~\cite{abu2022learning} and tactile guidance~\cite{she2021cable}. However, these approaches typically learn task-specific force patterns rather than generalizable force-compliant behaviors. SWIVL learns task-agnostic force regulation by incorporating screw-axis-based wrench decomposition into the reward structure.

\subsection{Hierarchical Integration of Cognitive and Physical Control}

\textbf{Residual Reinforcement Learning.}  To bridge the gap between high-level imitation policies and low-level reactive control, residual RL methods augment frozen behavior-cloned policies with learned corrective actions. Residual RL methods like ResiP~\cite{silver2022residual} augment frozen BC policies with learned corrective actions, protecting against catastrophic forgetting while handling distribution shifts. However, these focus on single-task manipulation without addressing multi-arm coordination or force regulation. SWIVL learns a standalone low-level policy that operates beneath arbitrary high-level planners.

\textbf{Hierarchical Control for Whole-Body Systems.} High-level planners (VLAs, teleoperation) generate semantic task specifications, but translating these into dynamically feasible whole-body motions requires robust low-level control. Hierarchical approaches separate high-level planning from low-level execution: LeVERB~\cite{radosavovic2024learning} learns latent action vocabularies translated by RL policies, while loco-manipulation frameworks~\cite{fu2023deep} use RL to track commands while maintaining stability. SWIVL adopts similar philosophy for dual-arm manipulation, where high-level planners provide pose commands and RL-trained low-level policy ensures physically feasible execution through explicit constraint and force modeling.