% Appendix: Learning Settings for SE(2) Implementation
\section{Learning Settings for SE(2) Implementation}
\label{app:learning_settings}

This appendix provides comprehensive implementation details for the SWIVL Low-Level Policy in the SE(2) planar manipulation setting, including network architecture, training configuration, and simulation environment specifications. All experiments are conducted in the BiarT (Bimanual Articulated manipulation) environment described in Section~\ref{sec:exp_setup}.

\subsection{Network Architecture}

The Low-Level Policy $\pi_\theta: \mathcal{O} \to \Delta(\mathcal{A})$ is implemented as a neural network with object-conditioned multi-stream architecture, employing Feature-wise Linear Modulation (FiLM) to inject object geometric structure throughout all feature processing stages.

\subsubsection{Input and Output Specifications}

\textbf{Observation Space:} $o_t \in \mathbb{R}^{30}$ (SE(2) planar setting)
\begin{itemize}
\item \textbf{Reference Twists} (6-dim): $\mathcal{V}_l^{\text{ref}}, \mathcal{V}_r^{\text{ref}} \in \mathbb{R}^3$
\item \textbf{Object Constraints} (6-dim): Body-frame screw axes $\mathcal{B}_l, \mathcal{B}_r \in \mathbb{R}^3$
\item \textbf{Wrench Feedback} (6-dim): Body wrenches $\mathcal{F}_l, \mathcal{F}_r \in \mathbb{R}^3$
\item \textbf{Proprioception} (12-dim): End-effector poses $(x_i, y_i, \theta_i) \in \mathbb{R}^3$ × 2 (6-dim), body twists $\mathcal{V}_i = (\omega_{z,i}, v_{x,i}, v_{y,i}) \in \mathbb{R}^3$ × 2 (6-dim)
\end{itemize}

Note: This corresponds to the SE(2) observation space detailed in Method Section 3.2.3 and Appendix~\ref{app:se2}.

\textbf{Input Normalization:} Each modality is normalized before being fed to its respective encoder to ensure balanced gradients and stable learning:

\begin{itemize}
\item \textbf{Reference Twists}: Twist components clipped to $[-v_{\max}, v_{\max}]$ then scaled by $s_{ref}$
\item \textbf{Object Constraints}: Screw axes are already unit-normalized
\item \textbf{Wrench Feedback}: Running normalization with exponential moving average: $\hat{\mathcal{F}} = (\mathcal{F} - \mu_{\mathcal{F}}) / (\sigma_{\mathcal{F}} + \epsilon)$ where $\mu_{\mathcal{F}}, \sigma_{\mathcal{F}}$ are updated online with decay $\alpha_{wrench}$
\item \textbf{Proprioception}: Poses clipped to workspace bounds $[-p_{\max}, p_{\max}] \times [-p_{\max}, p_{\max}] \times [-\pi, \pi]$ then scaled by $s_{pose}$; body twists clipped to $[-\dot{p}_{\max}, \dot{p}_{\max}]$ then scaled by $s_{vel}$
\end{itemize}

\textbf{Action Space:} $a_t \in \mathbb{R}^7$ (SE(2) planar setting)
\begin{itemize}
\item Per-arm damping coefficients for internal motion: $d_{l,\parallel}, d_{r,\parallel} \in \mathbb{R}$
\item Per-arm damping coefficients for bulk motion: $d_{l,\perp}, d_{r,\perp} \in \mathbb{R}$
\item Stiffness gains: $k_{p_l}, k_{p_r} \in \mathbb{R}$
\item Characteristic length scale: $\alpha \in \mathbb{R}$
\end{itemize}

These impedance variables parameterize the SE(2) screw-decomposed controller as detailed in Appendix~\ref{app:se2}.

\subsubsection{Multi-Stream Encoder Architecture}

\textbf{Object Structure Encoder (Conditioning Generator):}

The object encoder processes kinematic constraint information and generates a shared embedding that is then projected to stream-specific FiLM parameters:

$$
\begin{aligned}
h_{obj}^{(1)} &= \text{SiLU}(\text{LayerNorm}(W_{obj}^{(1)} x_{obj} + b_{obj}^{(1)})) \in \mathbb{R}^{64}, \\
e_{obj} &= \text{SiLU}(\text{LayerNorm}(W_{obj}^{(2)} h_{obj}^{(1)} + b_{obj}^{(2)})) \in \mathbb{R}^{128}
\end{aligned}
$$

where $x_{obj} \in \mathbb{R}^{6}$ contains body-frame screw axes $\mathcal{B}_l, \mathcal{B}_r \in \mathbb{R}^3$. The shared object embedding $e_{obj}$ is projected to layer-specific FiLM parameters via lightweight affine transformations:

$$
[\gamma_{s}^{(l)}, \beta_{s}^{(l)}] = W_{FiLM,s}^{(l)} e_{obj} + b_{FiLM,s}^{(l)} \in \mathbb{R}^{d_s} \times \mathbb{R}^{d_s}
$$

where $s \in \{\text{ref}, \text{wrench}, \text{proprio}, \text{fuse}\}$ denotes the stream, $l$ is the layer index, and $d_s$ is the feature dimension of that layer. This ensures dimensional compatibility between FiLM parameters and target features.

\textbf{Reference Motion Encoder:}

Processes reference twists with object-aware feature transformation:

$$
\begin{aligned}
h_{ref}^{(0)} &= \text{SiLU}(\text{LayerNorm}(W_{ref}^{(1)} x_{ref} + b_{ref}^{(1)})) \in \mathbb{R}^{128}, \\
h_{ref} &= \text{FiLM}(\text{LayerNorm}(W_{ref}^{(2)} h_{ref}^{(0)} + b_{ref}^{(2)}); \gamma_{ref}^{(1)}, \beta_{ref}^{(1)}) \in \mathbb{R}^{128}
\end{aligned}
$$

where $x_{ref} \in \mathbb{R}^{6}$ contains reference twists $\mathcal{V}_l^{\text{ref}}, \mathcal{V}_r^{\text{ref}}$. The policy network internally computes bulk-internal decomposition using projection operators.

\textbf{Wrench Encoder:}

Processes force-torque sensor feedback with object-aware feature transformation:

$$
\begin{aligned}
h_{wrench}^{(0)} &= \text{SiLU}(\text{LayerNorm}(W_{wrench}^{(1)} x_{wrench} + b_{wrench}^{(1)})) \in \mathbb{R}^{128}, \\
h_{wrench} &= \text{FiLM}(\text{LayerNorm}(W_{wrench}^{(2)} h_{wrench}^{(0)} + b_{wrench}^{(2)}); \gamma_{wrench}^{(1)}, \beta_{wrench}^{(1)}) \in \mathbb{R}^{128}
\end{aligned}
$$

where $x_{wrench} \in \mathbb{R}^{6}$ contains body wrenches $\mathcal{F}_l, \mathcal{F}_r$. The policy network internally computes productive-internal wrench decomposition using projection operators.

\textbf{Proprioception Encoder:}

Processes robot state information with higher capacity for rich state representation:

$$
\begin{aligned}
h_{proprio}^{(0)} &= \text{SiLU}(\text{LayerNorm}(W_{proprio}^{(1)} x_{proprio} + b_{proprio}^{(1)})) \in \mathbb{R}^{128}, \\
h_{proprio} &= \text{FiLM}(\text{LayerNorm}(W_{proprio}^{(2)} h_{proprio}^{(0)} + b_{proprio}^{(2)}); \gamma_{proprio}^{(1)}, \beta_{proprio}^{(1)}) \in \mathbb{R}^{128}
\end{aligned}
$$

where $x_{proprio} \in \mathbb{R}^{12}$ contains end-effector poses and velocities.

\subsubsection{Multi-Modal Fusion and Policy Head}

\textbf{Feature Fusion:}

Encoded features from all streams are concatenated and fused through object-conditioned layers:

$$
\begin{aligned}
\tilde{h} &= [h_{ref}, h_{wrench}, h_{proprio}] \in \mathbb{R}^{384}, \\
h_{fused}^{(1)} &= \text{SiLU}(\text{FiLM}(W_{fuse}^{(1)} \tilde{h} + b_{fuse}^{(1)}; \gamma_{fuse}^{(1)}, \beta_{fuse}^{(1)})) \in \mathbb{R}^{256}, \\
h_{context} &= \text{FiLM}(W_{fuse}^{(2)} h_{fused}^{(1)} + b_{fuse}^{(2)}; \gamma_{fuse}^{(2)}, \beta_{fuse}^{(2)}) \in \mathbb{R}^{256}
\end{aligned}
$$

\textbf{Action Decoder:}

The fused context is decoded into action distribution parameters:

$$
\begin{aligned}
h_{action} &= \text{SiLU}(W_{action}^{(1)} h_{context} + b_{action}^{(1)}) \in \mathbb{R}^{128}, \\
[\mu, \log\sigma] &= W_{action}^{(2)} h_{action} + b_{action}^{(2)} \in \mathbb{R}^{14}
\end{aligned}
$$

where $\mu \in \mathbb{R}^7$ and $\log\sigma \in \mathbb{R}^7$ parameterize a diagonal Gaussian action distribution $\pi_\theta(a|o) = \mathcal{N}(a; \mu(o), \text{diag}(\exp(\log\sigma(o))))$ for the 7-dimensional SE(2) impedance action space $(d_{l,\parallel}, d_{r,\parallel}, d_{l,\perp}, d_{r,\perp}, k_{p_l}, k_{p_r}, \alpha)$. The log standard deviation is clipped to $[\log(0.01), \log(10)]$ to prevent numerical instability.

\textbf{Positivity Constraint:} Since all impedance parameters must be strictly positive ($d_{i,\parallel}, d_{i,\perp}, k_{p_i}, \alpha \in \mathbb{R}^+$) for physical stability, the sampled actions from the Gaussian distribution are passed through a Softplus activation function:
$$
a_{\text{final}} = \text{Softplus}(a_{\text{sampled}}) = \log(1 + \exp(a_{\text{sampled}}))
$$
This ensures $a_{\text{final}} > 0$ for all components while maintaining differentiability for policy gradient updates. The Softplus function provides smooth gradients near zero, avoiding the non-differentiability issues of ReLU or absolute value, and naturally prevents negative damping or stiffness coefficients that would destabilize the impedance controller.

\subsubsection{Architectural Components}

\textbf{FiLM Layer:} Feature-wise Linear Modulation applies affine transformation based on object conditioning:
$$
\text{FiLM}(h; \gamma^{(obj)}, \beta^{(obj)}) = \gamma^{(obj)} \odot h + \beta^{(obj)}
$$

where $\gamma^{(obj)}, \beta^{(obj)} \in \mathbb{R}^{d}$ are stream- and layer-specific parameters projected from the shared object embedding $e_{obj}$ and modulate features element-wise. This enables object-specific feature transformation throughout the network while maintaining dimensional compatibility.

\textbf{Activation:} SiLU (Swish) for smooth gradients: $\text{SiLU}(x) = x \cdot \sigma(x)$

\textbf{Normalization:} LayerNorm with $\epsilon = 10^{-5}$: $\text{LayerNorm}(x) = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \odot \gamma_{LN} + \beta_{LN}$

Note: The learnable parameters $\gamma_{LN}$ and $\beta_{LN}$ in LayerNorm are distinct from the FiLM parameters $\gamma^{(obj)}$ and $\beta^{(obj)}$.

\subsection{Training Configuration}

\subsubsection{Reinforcement Learning Algorithm}

We train the Low-Level Policy using Proximal Policy Optimization (PPO) with clipped objective:

$$
L^{CLIP}(\theta) = \mathbb{E}_t \left[\min\left(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t\right)\right]
$$

where $r_t(\theta) = \frac{\pi_\theta(a_t|o_t)}{\pi_{\theta_{old}}(a_t|o_t)}$ is the probability ratio and advantages are computed via Generalized Advantage Estimation (GAE).

\subsubsection{Hyperparameters}

\textbf{Optimization:}
\begin{itemize}
\item \textbf{Optimizer:} Adam with $\beta_1 = 0.9$, $\beta_2 = 0.999$
\item \textbf{Learning rate:} $3 \times 10^{-4}$ with linear decay over training
\item \textbf{Gradient clipping:} Maximum norm 0.5
\item \textbf{Weight decay:} $10^{-4}$
\end{itemize}

\textbf{PPO Configuration:}
\begin{itemize}
\item \textbf{Rollout horizon:} 256 steps per worker
\item \textbf{Batch size:} 4096 transitions per iteration
\item \textbf{Mini-batch size:} 256 transitions per update
\item \textbf{Update epochs:} 10 epochs per batch
\item \textbf{Clip range:} $\epsilon = 0.2$
\item \textbf{Value loss coefficient:} 0.5
\item \textbf{Entropy coefficient:} $0.01 \to 0.001$ (linear annealing)
\end{itemize}

\textbf{GAE Configuration:}
\begin{itemize}
\item \textbf{Discount factor:} $\gamma = 0.99$
\item \textbf{GAE lambda:} $\lambda = 0.95$
\end{itemize}

\textbf{Policy Distribution:}
\begin{itemize}
\item \textbf{Type:} Diagonal Gaussian with state-dependent standard deviation
\item \textbf{Initial log std:} $\log\sigma_0 = -0.5$
\item \textbf{Action bounds:} $[-10, 10]$ for raw Gaussian samples before Softplus transformation (ensuring final positive actions in practical range $[\text{Softplus}(-10), \text{Softplus}(10)] \approx [4.5 \times 10^{-5}, 10.00]$)
\end{itemize}

\subsubsection{Initialization Strategy}

\textbf{Linear Layers:}
Xavier initialization with fan-averaging:
$$
W \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}}\right)
$$

\textbf{FiLM Generators:}
Initialize scale parameters near identity and shift parameters near zero to ensure stable initial conditioning:
$$
W_{\gamma} \sim \mathcal{N}(0, 0.01^2), \quad b_{\gamma} = 1, \quad W_{\beta} \sim \mathcal{N}(0, 0.01^2), \quad b_{\beta} = 0
$$

This ensures that FiLM conditioning initially approximates identity transformation, preventing disruption of gradient flow during early training.

\textbf{Action Head:}
Small-scale initialization to encourage near-zero initial actions:
$$
W_{action}^{(2)} \sim \mathcal{N}(0, 0.01^2), \quad b_{action}^{(2)} = 0
$$
