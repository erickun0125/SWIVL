% Algorithms for SWIVL Paper
% Include these in the main paper or supplementary material
%
% Convention: Following Modern Robotics (Lynch & Park)
% - Twist: V = [\omega, v_x, v_y]^T (angular velocity first!)
% - Wrench: F = [\tau, f_x, f_y]^T (torque first!)
% - Screw axis: B = [s_\omega, s_x, s_y]^T (angular component first!)

\begin{algorithm}[t]
\caption{SE(2) Task Space Impedance Control}
\label{alg:impedance_control}
\begin{algorithmic}[1]
\REQUIRE Current pose $T_{si}$, desired pose $T_{si}^d$, desired twist ${}^i V_i^d = [\omega^d, v_x^d, v_y^d]^T$, desired acceleration ${}^i \dot{V}_i^d$, current twist ${}^i V_i = [\omega, v_x, v_y]^T$, measured wrench ${}^i F_{\text{ext}} = [\tau, f_x, f_y]^T$, impedance gains $(D_d, K_d)$
\ENSURE Wrench command ${}^i F_i^{\text{cmd}} = [\tau, f_x, f_y]^T$

\STATE Compute pose error: $e_i \gets \log(T_{si}^{-1} T_{si}^d)$
\STATE Compute velocity error: ${}^i V_e \gets {}^i V_i - {}^i V_i^d$

\STATE // Compute robot dynamics in body frame
\STATE $\Lambda_b \gets$ Task space inertia matrix
\STATE $C_b \gets$ Task space Coriolis matrix
\STATE $\eta_b \gets$ Gravity compensation vector

\STATE // Impedance control law (model matching: $M_d = \Lambda_b$)
\STATE ${}^i F_i^{\text{cmd}} \gets \Lambda_b \cdot {}^i \dot{V}_i^d + C_b \cdot {}^i V_i + \eta_b$
\STATE \hspace{1.5cm} $+ D_d \cdot {}^i V_e + K_d \cdot e_i - {}^i F_{\text{ext}}$

\RETURN ${}^i F_i^{\text{cmd}}$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[t]
\caption{Screw-Decomposed Impedance Control}
\label{alg:screw_impedance}
\begin{algorithmic}[1]
\REQUIRE Current pose $T_{si}$, desired pose $T_{si}^d$, desired twist ${}^i V_i^d = [\omega^d, v_x^d, v_y^d]^T$, current twist ${}^i V_i = [\omega, v_x, v_y]^T$, measured wrench ${}^i F_{\text{ext}} = [\tau, f_x, f_y]^T$, screw axis $B_i = [s_\omega, s_x, s_y]^T$, impedance params $(D_\parallel, K_\parallel, D_\perp, K_\perp)$
\ENSURE Wrench command ${}^i F_i^{\text{cmd}} = [\tau, f_x, f_y]^T$

\STATE // Compute errors
\STATE $e_i \gets \log(T_{si}^{-1} T_{si}^d)$
\STATE ${}^i V_e \gets {}^i V_i - {}^i V_i^d$

\STATE // Screw decomposition of velocity error
\STATE $\alpha_V \gets B_i \cdot {}^i V_e$ \hfill // Parallel magnitude
\STATE $V_{e,\parallel} \gets \alpha_V \cdot B_i$ \hfill // Parallel component
\STATE $V_{e,\perp} \gets {}^i V_e - V_{e,\parallel}$ \hfill // Perpendicular component

\STATE // Screw decomposition of pose error
\STATE $\alpha_e \gets B_i \cdot e_i$
\STATE $e_{\parallel} \gets \alpha_e \cdot B_i$
\STATE $e_{\perp} \gets e_i - e_{\parallel}$

\STATE // Compute robot dynamics
\STATE $\Lambda_b \gets$ Task space inertia matrix
\STATE $C_b \gets$ Task space Coriolis matrix
\STATE $\eta_b \gets$ Gravity compensation vector

\STATE // Directional impedance wrenches
\STATE $F_{\parallel} \gets D_\parallel \cdot V_{e,\parallel} + K_\parallel \cdot e_{\parallel}$
\STATE $F_{\perp} \gets D_\perp \cdot V_{e,\perp} + K_\perp \cdot e_{\perp}$

\STATE // Total wrench command
\STATE ${}^i F_i^{\text{cmd}} \gets \Lambda_b \cdot {}^i \dot{V}_i^d + C_b \cdot {}^i V_i + \eta_b$
\STATE \hspace{1.5cm} $+ F_{\parallel} + F_{\perp} - {}^i F_{\text{ext}}$

\RETURN ${}^i F_i^{\text{cmd}}$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[t]
\caption{Hierarchical Control Loop}
\label{alg:hierarchical_control}
\begin{algorithmic}[1]
\REQUIRE High-level planner $\pi_{\text{HL}}$, RL impedance policy $\pi_{\text{RL}}$, impedance controller $\mathcal{C}$, environment $\mathcal{E}$
\ENSURE Episode trajectory

\STATE Initialize environment: $s_0 \gets \mathcal{E}.\text{reset}()$
\STATE $t \gets 0$, $t_{\text{HL}} \gets 0$, $t_{\text{RL}} \gets 0$

\WHILE{not done}
    \STATE // High-level planner (10 Hz)
    \IF{$t \bmod 10 = 0$}
        \STATE Get visual observation $o_t$
        \STATE $\{T_{sL}^{\text{goal}}, T_{sR}^{\text{goal}}\} \gets \pi_{\text{HL}}(o_t)$
        \STATE Generate minimum jerk trajectories:
        \STATE \hspace{0.5cm} $\text{Traj}_L \gets \text{MinJerk}(T_{sL}, T_{sL}^{\text{goal}}, \tau=1.0)$
        \STATE \hspace{0.5cm} $\text{Traj}_R \gets \text{MinJerk}(T_{sR}, T_{sR}^{\text{goal}}, \tau=1.0)$
        \STATE $t_{\text{HL}} \gets 0$
    \ENDIF

    \STATE // RL policy (10 Hz)
    \IF{$t \bmod 10 = 0$}
        \STATE Construct RL state:
        \STATE \hspace{0.5cm} $s_t^{\text{RL}} \gets [{}^L F_{\text{ext}}, {}^R F_{\text{ext}}, T_{sL}, T_{sR}, {}^L V_L, {}^R V_R,$
        \STATE \hspace{2cm} $T_{sL}^d, T_{sR}^d, {}^L V_L^d, {}^R V_R^d]$
        \STATE Get impedance parameters: $a_t \gets \pi_{\text{RL}}(s_t^{\text{RL}})$
        \STATE Decode to $(D_d, K_d)$ or $(D_\parallel, K_\parallel, D_\perp, K_\perp)$
        \STATE Update impedance controller gains: $\mathcal{C}.\text{set\_gains}(a_t)$
        \STATE $t_{\text{RL}} \gets 0$
    \ENDIF

    \STATE // Evaluate trajectories at current time
    \STATE $\{T_{sL}^d, {}^L V_L^d, {}^L \dot{V}_L^d\} \gets \text{Traj}_L.\text{evaluate}(t_{\text{HL}} \Delta t)$
    \STATE $\{T_{sR}^d, {}^R V_R^d, {}^R \dot{V}_R^d\} \gets \text{Traj}_R.\text{evaluate}(t_{\text{HL}} \Delta t)$

    \STATE // Low-level impedance control (100 Hz)
    \FOR{$i \in \{L, R\}$}
        \STATE Get current state: $T_{si}, {}^i V_i, {}^i F_{\text{ext}}$
        \STATE Compute wrench: ${}^i F_i^{\text{cmd}} \gets \mathcal{C}.\text{compute\_wrench}(T_{si}, T_{si}^d, {}^i V_i, {}^i V_i^d, {}^i \dot{V}_i^d, {}^i F_{\text{ext}})$
    \ENDFOR

    \STATE // Execute in environment
    \STATE $s_{t+1}, r_t, \text{done} \gets \mathcal{E}.\text{step}([{}^L F_L^{\text{cmd}}, {}^R F_R^{\text{cmd}}])$

    \STATE $t \gets t + 1$
    \STATE $t_{\text{HL}} \gets t_{\text{HL}} + 1$
    \STATE $t_{\text{RL}} \gets t_{\text{RL}} + 1$
\ENDWHILE

\RETURN Episode trajectory
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[t]
\caption{Impedance Variable Learning (PPO Training)}
\label{alg:ppo_training}
\begin{algorithmic}[1]
\REQUIRE High-level planner $\pi_{\text{HL}}$, impedance controller type $\mathcal{C}$, environment $\mathcal{E}$, total timesteps $T_{\text{max}}$
\ENSURE Trained impedance policy $\pi_{\text{RL}}$

\STATE Initialize policy network $\pi_{\text{RL}}$ and value network $V$
\STATE $t_{\text{total}} \gets 0$

\WHILE{$t_{\text{total}} < T_{\text{max}}$}
    \STATE // Collect rollout
    \STATE $\mathcal{D} \gets \{\}$ \hfill // Empty buffer
    \STATE $s_0 \gets \mathcal{E}.\text{reset}()$

    \FOR{$t = 0$ to $N_{\text{steps}}-1$}
        \STATE // Get RL state from environment
        \STATE $s_t^{\text{RL}} \gets$ Construct state from $({}^L F_{\text{ext}}, {}^R F_{\text{ext}}, T_{sL}, T_{sR}, \ldots)$

        \STATE // Sample action from policy
        \STATE $a_t \sim \pi_{\text{RL}}(\cdot | s_t^{\text{RL}})$

        \STATE // Execute hierarchical control (Algorithm \ref{alg:hierarchical_control})
        \STATE Update impedance gains with $a_t$
        \STATE Generate trajectories from $\pi_{\text{HL}}$
        \FOR{$k = 0$ to 9} \hfill // 10 impedance control steps per RL step
            \STATE Evaluate trajectory at time $k \Delta t$
            \STATE Compute wrench via $\mathcal{C}$
            \STATE $s', r', \text{done} \gets \mathcal{E}.\text{step}(\text{wrench})$
        \ENDFOR

        \STATE // Accumulate reward over 10 steps
        \STATE $r_t \gets \sum_{k=0}^{9} r_k'$
        \STATE $s_{t+1}^{\text{RL}} \gets s'$

        \STATE // Store transition
        \STATE $\mathcal{D} \gets \mathcal{D} \cup \{(s_t^{\text{RL}}, a_t, r_t, s_{t+1}^{\text{RL}})\}$

        \IF{done}
            \STATE break
        \ENDIF
    \ENDFOR

    \STATE // Compute advantages using GAE
    \STATE Compute returns $\hat{R}_t$ and advantages $\hat{A}_t$ for all $t$

    \STATE // PPO update
    \FOR{epoch $= 1$ to $N_{\text{epochs}}$}
        \FOR{minibatch $\mathcal{B} \subseteq \mathcal{D}$}
            \STATE // Policy loss (clipped objective)
            \STATE $L_{\text{policy}} \gets -\mathbb{E}_{\mathcal{B}}[\min(r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t)]$

            \STATE // Value loss
            \STATE $L_{\text{value}} \gets \mathbb{E}_{\mathcal{B}}[(V(s_t) - \hat{R}_t)^2]$

            \STATE // Total loss
            \STATE $L \gets L_{\text{policy}} + c_1 L_{\text{value}} - c_2 H[\pi_{\text{RL}}]$

            \STATE // Gradient update
            \STATE $\theta \gets \theta - \alpha \nabla_\theta L$
        \ENDFOR
    \ENDFOR

    \STATE $t_{\text{total}} \gets t_{\text{total}} + N_{\text{steps}}$
\ENDWHILE

\RETURN $\pi_{\text{RL}}$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[t]
\caption{Minimum Jerk Trajectory Generation}
\label{alg:min_jerk}
\begin{algorithmic}[1]
\REQUIRE Start pose $T_{\text{start}} \in \text{SE}(2)$, goal pose $T_{\text{goal}} \in \text{SE}(2)$, duration $\tau$
\ENSURE Trajectory function $\text{Traj}(t): [0, \tau] \to \text{SE}(2) \times \mathfrak{se}(2) \times \mathfrak{se}(2)$

\STATE // Compute pose displacement in se(2)
\STATE $\Delta p \gets \log(T_{\text{start}}^{-1} T_{\text{goal}}) \in \mathfrak{se}(2)$

\STATE // Define minimum jerk polynomial
\FUNCTION{$s$}{$t$}
    \STATE $\tau_n \gets t / \tau$ \hfill // Normalized time $\in [0, 1]$
    \RETURN $10\tau_n^3 - 15\tau_n^4 + 6\tau_n^5$ \hfill // Quintic polynomial
\ENDFUNCTION

\FUNCTION{$\dot{s}$}{$t$}
    \STATE $\tau_n \gets t / \tau$
    \RETURN $(30\tau_n^2 - 60\tau_n^3 + 30\tau_n^4) / \tau$
\ENDFUNCTION

\FUNCTION{$\ddot{s}$}{$t$}
    \STATE $\tau_n \gets t / \tau$
    \RETURN $(60\tau_n - 180\tau_n^2 + 120\tau_n^3) / \tau^2$
\ENDFUNCTION

\STATE // Trajectory evaluation at time $t$
\FUNCTION{evaluate}{$t$}
    \STATE // Pose interpolation (exponential map)
    \STATE $T(t) \gets T_{\text{start}} \cdot \exp(s(t) \cdot \Delta p)$

    \STATE // Velocity in spatial frame
    \STATE ${}^s V(t) \gets \dot{s}(t) \cdot \Delta p$

    \STATE // Velocity in body frame
    \STATE ${}^b V(t) \gets \text{Ad}_{T(t)^{-1}} \cdot {}^s V(t)$

    \STATE // Acceleration in spatial frame
    \STATE ${}^s \dot{V}(t) \gets \ddot{s}(t) \cdot \Delta p$

    \STATE // Acceleration in body frame
    \STATE ${}^b \dot{V}(t) \gets$ Transform ${}^s \dot{V}(t)$ to body frame

    \RETURN $\{T(t), {}^b V(t), {}^s V(t), {}^b \dot{V}(t)\}$
\ENDFUNCTION

\RETURN evaluate
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[t]
\caption{Screw Axis Extraction from Object Geometry}
\label{alg:screw_extraction}
\begin{algorithmic}[1]
\REQUIRE Object joint type (revolute or prismatic), joint configuration, end-effector poses $\{T_{sL}, T_{sR}\}$
\ENSURE Screw axes $\{B_L, B_R\}$ in body frames, $B_i = [s_\omega, s_x, s_y]^T$ (MR convention)

\STATE // Get joint axis in world frame
\IF{joint type is revolute}
    \STATE $\hat{z}_s \gets [0, 0, 1]^T$ \hfill // Rotation axis in SE(2)
    \STATE $p_{\text{joint}} \gets$ Joint center position in world frame

    \FOR{$i \in \{L, R\}$}
        \STATE // Transform to end-effector body frame
        \STATE $p_{\text{joint}}^i \gets T_{si}^{-1} \cdot p_{\text{joint}}$ \hfill // Joint position in body frame
        \STATE $d_i \gets \|p_{\text{joint}}^i\|$ \hfill // Distance from EE to joint

        \STATE // Revolute screw in SE(2): $B_i = [1, 0, d_i]^T$
        \STATE $\omega_i \gets 1$ \hfill // Angular component
        \STATE $v_i \gets [0, d_i]^T$ \hfill // Linear component
        \STATE $B_i \gets [\omega_i, v_i]^T$
    \ENDFOR

\ELSIF{joint type is prismatic}
    \STATE $\hat{v}_s \gets$ Joint direction in world frame (unit vector)

    \FOR{$i \in \{L, R\}$}
        \STATE // Transform to body frame
        \STATE $R_i \gets$ Rotation matrix from $T_{si}$
        \STATE $\hat{v}_i \gets R_i^{-1} \cdot \hat{v}_s$ \hfill // Direction in body frame

        \STATE // Prismatic screw in SE(2): $B_i = [0, \hat{v}_i]^T$
        \STATE $\omega_i \gets 0$ \hfill // No angular component
        \STATE $v_i \gets \hat{v}_i$ \hfill // Linear component (unit vector)
        \STATE $B_i \gets [\omega_i, v_i]^T$
    \ENDFOR
\ENDIF

\STATE // Normalize to ensure unit screw
\FOR{$i \in \{L, R\}$}
    \STATE $B_i \gets B_i / \|B_i\|$
\ENDFOR

\RETURN $\{B_L, B_R\}$
\end{algorithmic}
\end{algorithm}
